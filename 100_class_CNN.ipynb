{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "b6wHz4MVqkJE"
      },
      "outputs": [],
      "source": [
        "from preprocessing import * # custom library for mfcc extraction\n",
        "\n",
        "import os\n",
        "import keras\n",
        "import librosa\n",
        "import numpy as np\n",
        "import noisereduce as nr\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.constraints import maxnorm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import SGD, Adam, Adadelta\n",
        "from tensorflow.keras.layers import (Dense, Dropout, Flatten, Conv1D, Conv2D, GlobalMaxPooling2D, MaxPooling2D, AveragePooling2D, BatchNormalization, Activation, Bidirectional, TimeDistributed, SimpleRNN, GRU, LSTM, Input)\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mQo0S9pG2Pdq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving vectors of label - '0': 100%|██████████| 84/84 [00:04<00:00, 16.85it/s]\n",
            "Saving vectors of label - '1': 100%|██████████| 100/100 [00:05<00:00, 19.60it/s]\n",
            "Saving vectors of label - '2': 100%|██████████| 100/100 [00:05<00:00, 19.58it/s]\n",
            "Saving vectors of label - '3': 100%|██████████| 100/100 [00:05<00:00, 18.43it/s]\n",
            "Saving vectors of label - '4': 100%|██████████| 100/100 [00:05<00:00, 19.01it/s]\n",
            "Saving vectors of label - '5': 100%|██████████| 100/100 [00:05<00:00, 18.76it/s]\n",
            "Saving vectors of label - '6': 100%|██████████| 100/100 [00:05<00:00, 18.21it/s]\n",
            "Saving vectors of label - '7': 100%|██████████| 100/100 [00:05<00:00, 17.53it/s]\n",
            "Saving vectors of label - '8': 100%|██████████| 100/100 [00:05<00:00, 18.86it/s]\n",
            "Saving vectors of label - '9': 100%|██████████| 100/100 [00:05<00:00, 17.58it/s]\n",
            "Saving vectors of label - '10': 100%|██████████| 100/100 [00:06<00:00, 14.68it/s]\n",
            "Saving vectors of label - '11': 100%|██████████| 100/100 [00:05<00:00, 18.09it/s]\n",
            "Saving vectors of label - '12': 100%|██████████| 100/100 [00:06<00:00, 15.85it/s]\n",
            "Saving vectors of label - '13': 100%|██████████| 100/100 [00:06<00:00, 16.45it/s]\n",
            "Saving vectors of label - '14': 100%|██████████| 100/100 [00:05<00:00, 17.12it/s]\n",
            "Saving vectors of label - '15': 100%|██████████| 100/100 [00:05<00:00, 18.95it/s]\n",
            "Saving vectors of label - '16': 100%|██████████| 100/100 [00:05<00:00, 18.43it/s]\n",
            "Saving vectors of label - '17': 100%|██████████| 100/100 [00:05<00:00, 18.11it/s]\n",
            "Saving vectors of label - '18': 100%|██████████| 100/100 [00:05<00:00, 18.60it/s]\n",
            "Saving vectors of label - '19': 100%|██████████| 100/100 [00:05<00:00, 18.02it/s]\n",
            "Saving vectors of label - '20': 100%|██████████| 100/100 [00:05<00:00, 18.74it/s]\n",
            "Saving vectors of label - '21': 100%|██████████| 100/100 [00:05<00:00, 18.20it/s]\n",
            "Saving vectors of label - '22': 100%|██████████| 100/100 [00:05<00:00, 18.15it/s]\n",
            "Saving vectors of label - '23': 100%|██████████| 100/100 [00:05<00:00, 18.39it/s]\n",
            "Saving vectors of label - '24': 100%|██████████| 100/100 [00:05<00:00, 17.24it/s]\n",
            "Saving vectors of label - '25': 100%|██████████| 100/100 [00:05<00:00, 18.09it/s]\n",
            "Saving vectors of label - '26': 100%|██████████| 100/100 [00:05<00:00, 17.30it/s]\n",
            "Saving vectors of label - '27': 100%|██████████| 100/100 [00:05<00:00, 17.37it/s]\n",
            "Saving vectors of label - '28': 100%|██████████| 100/100 [00:05<00:00, 17.50it/s]\n",
            "Saving vectors of label - '29': 100%|██████████| 100/100 [00:06<00:00, 14.43it/s]\n",
            "Saving vectors of label - '30': 100%|██████████| 100/100 [00:06<00:00, 16.35it/s]\n",
            "Saving vectors of label - '31': 100%|██████████| 100/100 [00:06<00:00, 16.38it/s]\n",
            "Saving vectors of label - '32': 100%|██████████| 100/100 [00:05<00:00, 16.79it/s]\n",
            "Saving vectors of label - '33': 100%|██████████| 100/100 [00:05<00:00, 18.35it/s]\n",
            "Saving vectors of label - '34': 100%|██████████| 100/100 [00:06<00:00, 16.56it/s]\n",
            "Saving vectors of label - '35': 100%|██████████| 100/100 [00:05<00:00, 17.93it/s]\n",
            "Saving vectors of label - '36': 100%|██████████| 100/100 [00:06<00:00, 15.75it/s]\n",
            "Saving vectors of label - '37': 100%|██████████| 100/100 [00:06<00:00, 15.75it/s]\n",
            "Saving vectors of label - '38': 100%|██████████| 100/100 [00:06<00:00, 15.61it/s]\n",
            "Saving vectors of label - '39': 100%|██████████| 100/100 [00:07<00:00, 13.38it/s]\n",
            "Saving vectors of label - '40': 100%|██████████| 100/100 [00:07<00:00, 14.28it/s]\n",
            "Saving vectors of label - '41': 100%|██████████| 100/100 [00:05<00:00, 17.03it/s]\n",
            "Saving vectors of label - '42': 100%|██████████| 100/100 [00:05<00:00, 17.70it/s]\n",
            "Saving vectors of label - '43': 100%|██████████| 100/100 [00:06<00:00, 15.88it/s]\n",
            "Saving vectors of label - '44': 100%|██████████| 100/100 [00:06<00:00, 15.51it/s]\n",
            "Saving vectors of label - '45': 100%|██████████| 100/100 [00:05<00:00, 18.15it/s]\n",
            "Saving vectors of label - '46': 100%|██████████| 100/100 [00:06<00:00, 16.56it/s]\n",
            "Saving vectors of label - '47': 100%|██████████| 100/100 [00:06<00:00, 15.22it/s]\n",
            "Saving vectors of label - '48': 100%|██████████| 100/100 [00:06<00:00, 14.48it/s]\n",
            "Saving vectors of label - '49': 100%|██████████| 100/100 [00:06<00:00, 16.08it/s]\n",
            "Saving vectors of label - '50': 100%|██████████| 100/100 [00:05<00:00, 17.60it/s]\n",
            "Saving vectors of label - '51': 100%|██████████| 100/100 [00:05<00:00, 16.68it/s]\n",
            "Saving vectors of label - '52': 100%|██████████| 100/100 [00:05<00:00, 18.17it/s]\n",
            "Saving vectors of label - '53': 100%|██████████| 100/100 [00:05<00:00, 17.67it/s]\n",
            "Saving vectors of label - '54': 100%|██████████| 100/100 [00:05<00:00, 17.72it/s]\n",
            "Saving vectors of label - '55': 100%|██████████| 100/100 [00:05<00:00, 17.89it/s]\n",
            "Saving vectors of label - '56': 100%|██████████| 100/100 [00:05<00:00, 17.81it/s]\n",
            "Saving vectors of label - '57': 100%|██████████| 100/100 [00:05<00:00, 17.41it/s]\n",
            "Saving vectors of label - '58': 100%|██████████| 100/100 [00:05<00:00, 17.60it/s]\n",
            "Saving vectors of label - '59': 100%|██████████| 100/100 [00:05<00:00, 19.38it/s]\n",
            "Saving vectors of label - '60': 100%|██████████| 100/100 [00:05<00:00, 19.82it/s]\n",
            "Saving vectors of label - '61': 100%|██████████| 100/100 [00:05<00:00, 17.89it/s]\n",
            "Saving vectors of label - '62': 100%|██████████| 100/100 [00:05<00:00, 17.08it/s]\n",
            "Saving vectors of label - '63': 100%|██████████| 100/100 [00:05<00:00, 18.10it/s]\n",
            "Saving vectors of label - '64': 100%|██████████| 100/100 [00:05<00:00, 17.68it/s]\n",
            "Saving vectors of label - '65': 100%|██████████| 100/100 [00:05<00:00, 18.76it/s]\n",
            "Saving vectors of label - '66': 100%|██████████| 100/100 [00:05<00:00, 18.77it/s]\n",
            "Saving vectors of label - '67': 100%|██████████| 100/100 [00:06<00:00, 16.55it/s]\n",
            "Saving vectors of label - '68': 100%|██████████| 100/100 [00:05<00:00, 17.31it/s]\n",
            "Saving vectors of label - '69': 100%|██████████| 100/100 [00:06<00:00, 15.39it/s]\n",
            "Saving vectors of label - '70': 100%|██████████| 100/100 [00:06<00:00, 16.46it/s]\n",
            "Saving vectors of label - '71': 100%|██████████| 100/100 [00:05<00:00, 18.08it/s]\n",
            "Saving vectors of label - '72': 100%|██████████| 100/100 [00:05<00:00, 17.29it/s]\n",
            "Saving vectors of label - '73': 100%|██████████| 100/100 [00:05<00:00, 17.92it/s]\n",
            "Saving vectors of label - '74': 100%|██████████| 100/100 [00:05<00:00, 17.69it/s]\n",
            "Saving vectors of label - '75': 100%|██████████| 100/100 [00:06<00:00, 15.88it/s]\n",
            "Saving vectors of label - '76': 100%|██████████| 100/100 [00:06<00:00, 15.78it/s]\n",
            "Saving vectors of label - '77': 100%|██████████| 100/100 [00:05<00:00, 16.81it/s]\n",
            "Saving vectors of label - '78': 100%|██████████| 100/100 [00:05<00:00, 17.32it/s]\n",
            "Saving vectors of label - '79': 100%|██████████| 100/100 [00:05<00:00, 18.44it/s]\n",
            "Saving vectors of label - '80': 100%|██████████| 100/100 [00:05<00:00, 19.67it/s]\n",
            "Saving vectors of label - '81': 100%|██████████| 100/100 [00:05<00:00, 19.20it/s]\n",
            "Saving vectors of label - '82': 100%|██████████| 100/100 [00:05<00:00, 18.68it/s]\n",
            "Saving vectors of label - '83': 100%|██████████| 100/100 [00:05<00:00, 16.97it/s]\n",
            "Saving vectors of label - '84': 100%|██████████| 100/100 [00:05<00:00, 17.08it/s]\n",
            "Saving vectors of label - '85': 100%|██████████| 100/100 [00:06<00:00, 15.52it/s]\n",
            "Saving vectors of label - '86': 100%|██████████| 100/100 [00:06<00:00, 15.45it/s]\n",
            "Saving vectors of label - '87': 100%|██████████| 100/100 [00:06<00:00, 15.76it/s]\n",
            "Saving vectors of label - '88': 100%|██████████| 100/100 [00:06<00:00, 16.26it/s]\n",
            "Saving vectors of label - '89': 100%|██████████| 100/100 [00:06<00:00, 15.75it/s]\n",
            "Saving vectors of label - '90': 100%|██████████| 100/100 [00:06<00:00, 16.56it/s]\n",
            "Saving vectors of label - '91': 100%|██████████| 100/100 [00:06<00:00, 15.77it/s]\n",
            "Saving vectors of label - '92': 100%|██████████| 100/100 [00:05<00:00, 17.07it/s]\n",
            "Saving vectors of label - '93': 100%|██████████| 100/100 [00:06<00:00, 16.27it/s]\n",
            "Saving vectors of label - '94': 100%|██████████| 100/100 [00:07<00:00, 14.13it/s]\n",
            "Saving vectors of label - '95': 100%|██████████| 100/100 [00:06<00:00, 16.50it/s]\n",
            "Saving vectors of label - '96': 100%|██████████| 100/100 [00:05<00:00, 16.96it/s]\n",
            "Saving vectors of label - '97': 100%|██████████| 100/100 [00:05<00:00, 17.02it/s]\n",
            "Saving vectors of label - '98': 100%|██████████| 100/100 [00:06<00:00, 15.31it/s]\n",
            "Saving vectors of label - '99': 100%|██████████| 100/100 [00:06<00:00, 14.45it/s]\n"
          ]
        }
      ],
      "source": [
        "max_len = 39\n",
        "coefficients = 13\n",
        "\n",
        "labels=[]\n",
        "for i in range (100):\n",
        "    labels.append(str(i))\n",
        "\n",
        "# extracting mfccs and storing them as numpy array\n",
        "# path = 'path_to_the_folder_where_raw_audio_files_are_stored'\n",
        "# dest = 'path_to_where_extracted_mfccs_will_be_stored_as_numpy_array'\n",
        "\n",
        "save(path='./Bangla_Spoken_Numbers/', dest='./mfcc100/', labels=labels, feature='mfcc', max_len=max_len, coefficients=coefficients)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iJtCcpZ22UFU",
        "outputId": "72d08e48-547a-4078-cc5a-2f5b5f6a6421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train shape:  (7188, 39, 39)   (7188,)\n",
            "validation shape:  (799, 39, 39)   (799,)\n",
            "test shape:  (1997, 39, 39)   (1997,)\n",
            "\n",
            "\n",
            "[60, 71, 61, 80, 77, 78, 67, 79, 64, 76, 70, 70, 69, 74, 75, 67, 69, 75, 81, 66, 78, 71, 79, 69, 77, 69, 70, 75, 72, 79, 66, 75, 76, 67, 63, 72, 69, 68, 74, 73, 69, 76, 74, 64, 68, 75, 75, 74, 70, 72, 77, 68, 71, 70, 69, 71, 69, 67, 74, 80, 74, 73, 74, 77, 80, 73, 73, 71, 64, 70, 76, 74, 72, 62, 73, 77, 72, 75, 70, 73, 75, 78, 71, 65, 66, 72, 75, 77, 73, 57, 76, 74, 80, 72, 68, 68, 74, 71, 74, 65]\n",
            "\n",
            "\n",
            "[19, 22, 30, 13, 17, 15, 24, 16, 26, 18, 18, 21, 21, 17, 17, 22, 21, 19, 13, 26, 16, 18, 18, 18, 19, 22, 20, 19, 22, 18, 24, 17, 18, 20, 25, 23, 25, 25, 17, 17, 19, 16, 15, 27, 23, 24, 14, 21, 18, 21, 15, 23, 20, 23, 26, 23, 22, 26, 22, 14, 18, 21, 17, 19, 12, 17, 20, 23, 26, 23, 21, 16, 16, 29, 19, 16, 23, 20, 22, 20, 19, 17, 22, 23, 24, 18, 20, 14, 17, 29, 15, 14, 16, 20, 25, 24, 18, 18, 12, 26]\n",
            "\n",
            "\n",
            "[5, 7, 9, 7, 6, 7, 9, 5, 10, 6, 12, 9, 10, 9, 8, 11, 10, 6, 6, 8, 6, 11, 3, 13, 4, 9, 10, 6, 6, 3, 10, 8, 6, 13, 12, 5, 6, 7, 9, 10, 12, 8, 11, 9, 9, 1, 11, 5, 12, 7, 8, 9, 9, 7, 5, 6, 9, 7, 4, 6, 8, 6, 9, 4, 8, 10, 7, 6, 10, 7, 3, 10, 12, 9, 8, 7, 5, 5, 8, 7, 6, 5, 7, 12, 10, 10, 5, 9, 10, 14, 9, 12, 4, 8, 7, 8, 8, 11, 14, 9]\n"
          ]
        }
      ],
      "source": [
        "# Loading train set and test set\n",
        "X, y = load_data('./mfcc100/', labels=labels, val_size=0, test_size=0)\n",
        "\n",
        "test_size = 0.2\n",
        "val_size = 0.1\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=45)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=45)\n",
        "\n",
        "print('train shape: ', X_train.shape, ' ', y_train.shape)\n",
        "print('validation shape: ', X_val.shape, ' ', y_val.shape)\n",
        "print('test shape: ', X_test.shape, ' ', y_test.shape)\n",
        "\n",
        "print('\\n')\n",
        "no_class = []\n",
        "for i in range(100):\n",
        "    no_class.append(0)\n",
        "for i in y_train:\n",
        "    no_class[int(i)] += 1\n",
        "print(no_class)\n",
        "\n",
        "print('\\n')\n",
        "no_class = []\n",
        "for i in range(100):\n",
        "    no_class.append(0)\n",
        "for i in y_test:\n",
        "    no_class[int(i)] += 1\n",
        "print(no_class)\n",
        "\n",
        "print('\\n')\n",
        "no_class = []\n",
        "for i in range(100):\n",
        "    no_class.append(0)\n",
        "for i in y_val:\n",
        "    no_class[int(i)] += 1\n",
        "print(no_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uLGUpKdxLJyl"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABPsAAAJNCAYAAABHp3BlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAvrUlEQVR4nO3dfZBs6V0f9u+vZ+7d1csKLbuLrZKIJEB2lcA2jhaBHccIlEIrcCQlVlLCSZCCTMU2JH+ESoAiFbsMpIJJ+V2EbNkqcCVVghDb2jhCsgISTuJISIDMIkCwFmAkC5QVL0Ir7b13pn/5Y3pmWlv7MvelnzPnzOdT1bU9p7vP7/R5+pzb+/T3PE91dwAAAACA+VtNvQEAAAAAwK2hsw8AAAAAFkJnHwAAAAAshM4+AAAAAFgInX0AAAAAsBA6+wAAAABgIfZ3sdK7P+eZ/fzPu2sXq2a0qqm3gFuhe+ot2L0p3uNFOD4uwnu8KC7CeQAAYOF+5qF/9XB33zP1dpw3L1k9oz/Zh0NqPZQr7+ju+4YUu0E76ex7/ufdlf/nb33nLlbNaKsLEP6cojNj9P90r9dj6yXJesyJ9rTeBO9xiuNjtTe43gU4B1wUUxwjAADcUk/7uv/s16fehvPok32Yv7n//CG1/szBL989pNBN2ElnHwAAAAAMUUldGhTkORhT5maIbAAAAADAQkj2AQAAADBbVZXVvvHGj0n2AQAAAMBC6OwDAAAAgIVwGS8AAAAA81VJXZJnO2ZPAAAAAMBCSPYBAAAAMF8VE3RskewDAAAAgIWQ7AMAAABgviqpS5J9xyT7AAAAAGAhJPsAAAAAmK2qMmbflt109lUlqwsQGqzBH6TDw7H1kmS9HluvB9dLkprgszr6+NjbG1svGd+W3WPrJePPAck0bXkRjD6/XpRzHQAAMJxkHwAAAADzZcy+z+JnfgAAAABYCMk+AAAAAOarYsy+LZJ9AAAAALAQkn0AAAAAzFYlqT3JvmOSfQAAAACwEJJ9AAAAAMxXJSvJvhOSfQAAAACwEDr7AAAAAGAhXMYLAAAAwIxVauUy3mOSfQAAAACwEJJ9AAAAAMxXJbUnz3bMngAAAACAhdhNsq87Wa93smoWrvQ/70T3+Jr7l5Zdbyqj23KKc3lPUXPwfi3jiQAAwK1SSVZ7vmMf07MCAAAAAAthzD4AAAAA5qtiNt4tkn0AAAAAsBCSfQAAAADMWBmzb4tkHwAAAAAshGQfAAAAALNVlZRk3wnJPgAAAABYCMk+AAAAAGatVvJsx+wJAAAAAFgIyT4AAAAA5quSWhmz75hkHwAAAAAshM4+AAAAAFgIl/ECAAAAMGOV1Z7LeI/tprOvKtnb28mqGax76i1grg4Px9br9dh6SVIXIBw9xX6dwsq/WQAAwDJI9gEAAAAwW2WCjs9yAWIpAAAAAHAxSPYBAAAAMGu1kmc7Zk8AAAAAwEJI9gEAAAAwX8bs+yySfQAAAACwEJJ9AAAAAMxYZbUn2XdMsg8AAAAAFkKyDwAAAIDZKmP2fRbJPgAAAABYCMk+AAAAAGatVvJsx+wJAAAAAFgIyT4AAAAA5suYfZ9ld519JTR4y60PJ6i5Hl+TW68vQDt2T1B0gv06+ty6f2lsvWSafz9Gn18PD8bWm4LvAQAAMAnfxAEAAABgIVzGCwAAAMCMlct4t0j2AQAAAMBCSPYBAAAAMGuSfack+wAAAABgIST7AAAAAJitqqRW8mzH7AkAAAAAWAjJPgAAAABmbbVnzL5jkn0AAAAAsBCSfQAAAADMV5XZeLdI9gEAAADAQkj2AQAAADBrZuM9ZU8AAAAAwEJI9t2Mvb2x9daHY+slSa/H1qsJ+p9Ht+MUJvjoDP+8HlwbWy9JVhN8dvYHHyPdY+slSU/wgT08GF9ztCnOrwAAMEBVjNm3xTd/AAAAAFgIyT4AAAAAZk2y75RkHwAAAAAshM4+AAAAAFgIl/ECAAAAMGOVWsmzHbMnAAAAAGDHqur7quqXqurnquofVdWztx77jqp6qKo+VFWv2Fp+32bZQ1X17Wepo7MPAAAAgPmqowk6RtzOvElVL6uqH3zM4ncm+ZLu/qNJfjnJd2ye++Ikr0vyxUnuS/L9VbVXVXtJ3pTklUlenOTrN899Ujr7AAAAAGDHuvufdvfB5s/3JHne5v6rk7ylu690968meSjJSze3h7r7w919NclbNs99UsbsAwAAAGDGZjlm3zcm+eHN/efmqPPv2Ec2y5LkNx6z/MufasU6+wAAAADgbO6uqvdv/X1/d99//EdVvTfJbUmemeRzq+oDm4e+rbvfsXnOdyY5SPK/7GIDdfYBAAAAMG919vH0btLD3X3vEz3Y3V9+tDn1siRv6O43bD9eVW9I8meSvLy7e7P4o0k+f+tpz9ssy5Msf0KzyzgCAAAAwNxU1X1J/uskr+ruT2899ECS11XVbVX1wiQvSvJTSd6X5EVV9cKqupyjSTweeKo6kn0AAAAAzFZtZuOdgb+bo0t831lHScT3dPdf6O4PVtWPJPmFHF3e+83dfZgkVfUtSd6RZC/Jm7v7g09VZDedfet18uinn/p5czd68MdLl8fWS5I9/cGLML+BSq/f028fX3Nvb3zNa1fH1js8eOrn3GonSXZurfXYcnUBzjsAAPAEuvvdSd79mGVf9CTP/54k3/M4y9+W5G3XU1tPDgAAAACzNsPZeHfGngAAAACAhZDsAwAAAGC+quYyZt8Qkn0AAAAAsBCSfQAAAADMmjH7TtkTAAAAALAQOvsAAAAAYCFcxgsAAADArJmg45RkHwAAAAAshGQfAAAAALNVJdm3TbIPAAAAABZCsg8AAACAGatkJc92zJ4AAAAAgIXYTbKvLkiPag2+Hrx7bL0kOTwYX3O0ugCf1Sns7Q0t1/vjg8p1MMHxsV6PrTfJ8TH4PU5h/9L4mqPb8uDa2HoAAFxoNbqP5hzTywEAAAAAC2HMPgAAAADmq5K6CFeYnpE9AQAAAAALIdkHAAAAwIxVamXMvmOSfQAAAACwEJJ9AAAAAMxXJTFm3wl7AgAAAAAWQrIPAAAAgFkzZt8pyT4AAAAAWAjJPgAAAABmq1Kpkmc7Zk8AAAAAwELo7AMAAACAhXAZLwAAAADzVUlM0HFiJ519vdpLP/PZu1j1ubK+fNvQenuPfHJovSTJwbWx9WqCg3M1QcB19Ps8PBxbL0muPDq23sc/NrZekvV6Pbzm6q57xha8dHlsvSRZTVCze2y5KfZrj/281uh/PwAAgCSSfQAAAADMXE0R5Dmn7AkAAAAAWAjJPgAAAABmrYzZd0KyDwAAAAAWQrIPAAAAgPmqSkqe7Zg9AQAAAAALIdkHAAAAwKwZs++UZB8AAAAALIRkHwAAAADztpJnO2ZPAAAAAMBCSPYBAAAAMFtVlSpj9h2T7AMAAACAhZDsAwAAAGDejNl3YiedfdWdunZlF6s+V/YOr40tePXRsfWSZHQMtiY4OKeI+naPrbc+HFsvSfb2hparO+8aWi9JJgmJ71+aoupY166Or9nroeUmucTAlx8AALgQfPMHAAAAgIVwGS8AAAAAs1YrE3Qck+wDAAAAgIWQ7AMAAABgvqqmmQPgnLInAAAAAGAhJPsAAAAAmDdj9p2Q7AMAAACAhZDsAwAAAGDWyph9J+wJAAAAAFgIyT4AAAAA5qtizL4tkn0AAAAAsBCSfQAAAADMWKVW8mzH7AkAAAAAWIgdJfs6WR/uZtXnyPr2Z4wtePlpY+slWT36yNiChwdj610UhxMcjwfXxtccbf/S+Jrr9dh63WPrJUlNMNbG4LZcP/2OofWOio797KyuPDq0HgAAF9wU/x9xTkn2AQAAAMBCGLMPAAAAgPmqJMbsO2FPAAAAAMBC6OwDAAAAgIVwGS8AAAAAM1Ym6Ngi2QcAAAAACyHZBwAAAMCslQk6TtgTAAAAALAQkn0AAAAAzFclKXm2Y/YEAAAAACyEZB8AAAAAM1bJymy8xyT7AAAAAGAhJPsAAAAAmK1KUsbsO7Gbzr7u5ODaTlZ9nqx+7+GxBS9dHlsvSVZ742uOduXR8TW7x9bbm6Ad9wf/lnD59rH1kvT+peE169rVsQXXh2PrJcl6Pb5mj61ZVz49tN5RUV9+AADgIpDsAwAAAGC+Ksbs2+JnfgAAAABYCMk+AAAAAGasDFuzxZ4AAAAAgIWQ7AMAAABg3sqYfcck+wAAAABgIXT2AQAAAMBCuIwXAAAAgHlbybMdsycAAAAAYCEk+wAAAACYr6qk5NmO2RMAAAAAsBCSfQAAAADM26qm3oJzYyedfX1wkPXDH9/Fqs+VT/3SQ0PrXX72s4bWS5Lbv+gLxxZ8xh1j6yXJ/qXxNdeHQ8v10585tF6SXLnzuUPrPfKMe4bWS5I7fv9fD695+Td/dWzBvQl+E7p0eXzNwcfkT7ziu4fWm8LL3/qtU28CAABcSJJ9AAAAAMybMftO2BMAAAAAsBCSfQAAAADMWxmz75hkHwAAAAAshGQfAAAAAPNVlazk2Y7ZEwAAAACwEDr7AAAAAJi3qjG3m9rE+tyqemdV/crmv3feonf/WXT2AQAAAMAtVFUvq6offMzib0/y4939oiQ/vvn7ljNmHwAAAADzVrPIs706ycs2938oybuTfNutLjKLPQEAAAAAM/cHuvtjm/u/meQP7KKIZB8AAAAAnM3dVfX+rb/v7+77j/+oqvcmuS3JM5N8blV9YPPQZyX4ururqnexgTr7AAAAAJivqmQ17OLVh7v73id6sLu//GiT6mVJ3tDdbzh+rKp+q6qe090fq6rnJPn4LjbQZbwAAAAAsHsPJHn95v7rk7x1F0V2kuyr/f2s7vmDu1j1ufKsC/AeB/aMT2dvb3zNwfu1Hv3M0HpJcvtHPzS23voXh9ZLMs3xsZrg8zra4cHUW7BzL//f/6vxRdfr8TUBAGCUqqm34Cz++yQ/UlVvTPLrSf7DXRRxGS8AAAAA3ELd/e4czba7vewTSV6+69o6+wAAAACYt7oAVyaekT0BAAAAAAsh2QcAAADAjNVcxuwbQrIPAAAAABZCsg8AAACA+aokK3m2Y/YEAAAAACyEZB8AAAAAs9VJ2ph9JyT7AAAAAGAhJPsAAAAAmLFKSp7tmD0BAAAAAAsh2QcAAADAvEn2nbAnAAAAAGAhdpPsq0pWF6AfcfRML4eHY+slyXo9tl4PrpdM0/s/+vjY2xtbLxnflt1j6yXjzwHJNG15EYw+v16Ucx0AADCcy3gBAAAAmLWeIoxxTvmZHwAAAAAWQrIPAAAAgPmqMmzNFnsCAAAAABZCsg8AAACAeTNm3wnJPgAAAABYCMk+AAAAAOZtJc92zJ4AAAAAgIWQ7AMAAABgxiptzL4Tkn0AAAAAsBCSfQAAAADMVyUpebZj9gQAAAAALIRkHwAAAACz1pJ9J3bT2dedrNc7WTUL5+Dcje7xNfcvLbveVEa35RTn8p6i5uD9avBgAABgRyT7AAAAAJix8oP6FjEqAAAAAFgInX0AAAAAsBAu4wUAAABg1kzQccqeAAAAAICFkOwDAAAAYN5M0HFCsg8AAAAAFkKyDwAAAID5qkqM2XfCngAAAACAhZDsAwAAAGC2Okkbs++EZB8AAAAALIRkHwAAAADzZsy+E/YEAAAAACzEbpJ9Vcne3k5WzWDdU28Bc3V4OLZer8fWSy7GL0dT7NcprPybBQAAc9YxZt+xC/B/qgAAAABwMRizDwAAAIAZq/RFuPLqjOwJAAAAAFgIyT4AAAAA5k2y74Q9AQAAAAALobMPAAAAABbCZbwAAAAAzFclXTX1Vpwbkn0AAAAAsBCSfQAAAADMVqfSJug4YU8AAAAAwEJI9gEAAAAwb8bsOyHZBwAAAAALIdkHAAAAwKwZs+/U7jr77ORbb304Qc31+Jrcen0B2rF7gqIT7NfR59b9S2PrJdP8+zH6/Hp4MLbeFHwPAACASUj2AQAAADBjlY4x+4752R0AAAAAFkKyDwAAAIBZM2bfKXsCAAAAABZCsg8AAACA+aokZcy+Y5J9AAAAALAQkn0AAAAAzFil5dlO2BMAAAAAsBA6+wAAAABgIVzGCwAAAMBsdZI2QccJyT4AAAAAWAjJPgAAAABmrUue7Zg9AQAAAAALIdl3M/b2xtZbH46tlyS9Hltvip740e04hQk+OsM/rwfXxtZLktUEn539wcdI99h6SdITfGAPD8bXHM0vnQAALFjHmH3HfPMHAAAAgIWQ7AMAAABgxsqYfVvsCQAAAABYCMk+AAAAAGaty5h9xyT7AAAAAGAhJPsAAAAAmK2O2Xi3SfYBAAAAwEJI9gEAAAAwX2U23m32BAAAAAAshGQfAAAAALNmzL5Tkn0AAAAAsBA6+wAAAABgIVzGCwAAAMCsmaDj1G46+9br5NFP72TV58pq8Afp0uWx9ZJkT3/wIoz+rE7h6bePr7m3N77mtatj6x0ejK2XJN3ja14I67HlfNkCAIBJ6MkBAAAAYNZM0HHKz+4AAAAAsBCSfQAAAADMVqeM2bfFngAAAACAhZDsAwAAAGDWjNl3SrIPAAAAABZCZx8AAAAAs9ZVQ27Xo6r+o6r6uap6sKr+eVX9sa3H7quqD1XVQ1X17VvLX1hV790s/+Gquny9+0JnHwAAAADcer+a5Cu7+48k+a4k9ydJVe0leVOSVyZ5cZKvr6oXb17zvUn+Rnd/UZLfSfLG6y2qsw8AAACAWeuuIbfr26b+5939O5s/35PkeZv7L03yUHd/uLuvJnlLkldXVSX56iQ/unneDyV5zfXuC519AAAAALBbb0zyY5v7z03yG1uPfWSz7K4kv9vdB49Zfl3MxgsAAADAjFV6XJ7t7qp6/9bf93f3/U/2gqr6qhx19v2pnW7Zhs4+AAAAADibh7v73id6sKq+Ock3bf782iR3J/l7SV7Z3Z/YLP9oks/fetnzNss+keTZVbW/SfcdL78uLuMFAAAAYLY6SaeG3J5yW7rf1N1f2t1fmqOQ3T9M8p909y9vPe19SV60mXn3cpLXJXmguzvJu5K8dvO81yd56/Xuj90k+6qS1QXoR7zOKZdvWvfYeklyePDUz5m7ugCf1Sns7Q0t1/vjg8p1MMHxsV6PrTfJ8TH4PU5h/9L4mqPb8uDa2HoAAHD+/Lc5Gofv+4/m3shBd9/b3QdV9S1J3pFkL8mbu/uDm9d8W5K3VNV3J/nZJH//eou6jBcAAACAWTtL6m607v7zSf78Ezz2tiRve5zlH87RbL03TKQJAAAAABZCZx8AAAAALITLeAEAAACYtfN4Ge9UJPsAAAAAYCEk+wAAAACYsZLs2yLZBwAAAAALIdkHAAAAwKx1S/Ydk+wDAAAAgIWQ7AMAAABgtjpm490m2QcAAAAACyHZBwAAAMCsSfadkuwDAAAAgIXYSbKvV3vpZz57F6s+V9aXbxtab++RTw6tlyQ5uDa2Xk3QE7+aoM979Ps8PBxbL0muPDq23sc/NrZekvV6Pbzm6q57xha8dHlsvSRZTVCze2y5KfZrj/281uh/PwAAuNAk+05J9gEAAADAQhizDwAAAIAZq3RL9h2T7AMAAACAhZDsAwAAAGC2OsnamH0nJPsAAAAAYCF09gEAAADAQriMFwAAAIBZa5fxnpDsAwAAAICFkOwDAAAAYL466ZbsOybZBwAAAAALIdkHAAAAwKwZs++UZB8AAAAALIRkHwAAAAAzVsbs27KTzr7qTl27sotVnyt7h9fGFrz66Nh6SVKDD5aaIGw6+j0mSffYeuvDsfWSZG9vaLm6866h9ZJMExLfvzRF1bGuXR1fs9dDy9UU552VMD8AAFwEkn0AAAAAzFbHmH3b/MwPAAAAAAsh2QcAAADArBmz75RkHwAAAAAshGQfAAAAALM2dsq9802yDwAAAAAWQrIPAAAAgFkzZt8pyT4AAAAAWAidfQAAAACwEC7jBQAAAGC2OpWOy3iPSfYBAAAAwEJI9gEAAAAwayboOCXZBwAAAAALsaNkXyfrw92s+hxZ3/6MsQUvP21svSSrRx8ZW/DwYGy9i+JwguPx4Nr4mqPtXxpfc70eW697bL0kqQl+kRvcluun3zG03lHRsZ+d1ZVHh9YDAOBiM2bfKck+AAAAAFgIY/YBAAAAMF+drCe4KOm8kuwDAAAAgIWQ7AMAAABgtjrG7Nsm2QcAAAAACyHZBwAAAMCsdUv2HZPsAwAAAICFkOwDAAAAYNbabLwnJPsAAAAAYCEk+wAAAACYscrabLwnJPsAAAAAYCF09gEAAADAQriMFwAAAIDZ6iTdLuM9tpvOvu7k4NpOVn2erH7v4bEFL10eWy9JVnvja4525dHxNUdPE7Q3QTvuD/4t4fLtY+sl6f1Lw2vWtatjC64Px9ZLkvV6fM0eW7OufHpovaOiwvwAAHARSPYBAAAAMGujMzXnmZ/5AQAAAGAhJPsAAAAAmLWOMfuOSfYBAAAAwEJI9gEAAAAwX52sjdl3QrIPAAAAABZCsg8AAACA2eok3cbsOybZBwAAAAALIdkHAAAAwKy1MftOSPYBAAAAwEJI9gEAAAAwa+sYs++YZB8AAAAALIRkHwAAAACzZsy+Uzvp7OuDg6wf/vguVn2ufOqXHhpa7/KznzW0XpLc/kVfOLbgM+4YWy9J9i+Nr7k+HFqun/7MofWS5Mqdzx1a75Fn3DO0XpLc8fv/enjNy7/5q2ML7k3wm9Cly+NrDj4mf+IV3z203hRe/tZvnXoTAADgQnIZLwAAAAAshMt4AQAAAJitTqXbBB3HJPsAAAAAYCEk+wAAAACYr07WJug4IdkHAAAAAAsh2QcAAADArLVk3wnJPgAAAABYCMk+AAAAAGatYzbeY5J9AAAAALAQkn0AAAAAzFbHbLzbJPsAAAAAYCEk+wAAAACYNbPxnpLsAwAAAICF2Emyr/b3s7rnD+5i1efKsy7Ae7wQ9i+Nr7l3+9BydXg4tF6S3P7//frYer/14aH1kiRXr4yvOfrnqoODsfWSpCaYRWs19revl7/1W4fWAwCApZPsOyXZBwAAAAALYcw+AAAAAGarO1n3BFcInVOSfQAAAACwEDr7AAAAAGAhXMYLAAAAwKyZoOOUZB8AAAAA7EhVfVlVHVTVa7eWvb6qfmVze/3W8pdU1YNV9VBV/e2quu7BCHX2AQAAADBr3WNu16uq9pJ8b5J/urXsc5P85SRfnuSlSf5yVd25efh/TPJNSV60ud13vTV19gEAAADAbvznSf63JB/fWvaKJO/s7t/u7t9J8s4k91XVc5I8q7vf092d5B8kec31FjRmHwAAAACztj6HY/ZV1XOT/HtJvirJl2099Nwkv7H190c2y567uf/Y5ddFZx8AAAAAnM3dVfX+rb/v7+77n+C5fzPJt3X3+gaG3rthOvsAAAAAmK1O0j2sM+3h7r73iR6sqm/O0Zh7SfI5Sd6y6ei7O8nXVtVBko8mednWy56X5N2b5c97zPKPXu8G6uwDAAAAgFugu9+U5E2PXV5VP5jkn3T3P95M0PHfbU3K8TVJvqO7f7uqPllVX5HkvUm+Icnfud5t0NkHAAAAwHzd4Ey5U9l06n1XkvdtFv3V7v7tzf2/lOQHkzwtyY9tbtdlN519Vcne3k5W/YSmaNW9wX2lqwkmT16vx9ccbYrPzuHh2Hqjj8ckPXA8giSpg6HlprM/+LxTE5x3Jvi8Dn+f68HngORinM8BAOCc6u43PObvNyd58+M87/1JvuRmakn2AQAAADBr53E23qlMENkAAAAAAHZBsg8AAACA2TqajXfqrTg/JPsAAAAAYCEk+wAAAACYNcm+U5J9AAAAALAQOvsAAAAAYCFcxgsAAADArK1dxntCsg8AAAAAFkKyDwAAAID5ahN0bJPsAwAAAICFkOwDAAAAYLY6yXo99VacH7vp7OtODq7tZNVPqCYIKR4+Or4mt94UWd+qsfUGH45JUqOPyf1LY+slydOeMbxkT/E+R+vx/0rX4eHwmgAAALsg2QcAAADArBmz75Qx+wAAAABgIST7AAAAAJg1yb5Tkn0AAAAAsBCSfQAAAADMVneyluw7IdkHAAAAAAsh2QcAAADArLVB+05I9gEAAADAQkj2AQAAADBrgn2nJPsAAAAAYCF09gEAAADAQriMFwAAAIBZW6+n3oLzQ7IPAAAAABZid8m+0SMj1thySZK9wcHI1d7YeklSU+zYwVbj+7z7IuzXGrtfa304tF6SSX46qoNrYwtOMcptT/CTnNF8AQBgtrp9pd8m2QcAAAAAC2HMPgAAAABmbS3Zd0KyDwAAAAAWQrIPAAAAgFkzZt8pyT4AAAAAWAjJPgAAAABmrQ3ad0KyDwAAAAAWQrIPAAAAgNnqNhvvNsk+AAAAAFgIyT4AAAAAZs1svKck+wAAAABgIST7AAAAAJi1tUH7Tkj2AQAAAMBCLCfZ1+vxNVeXxtarGltvClNcZH94OLzk8Jac4vgY3ZaHB2PrJZN8dobv1ynOO/uDz61T1JzimAQAAC6E5XT2AQAAAHDhdEzQsc1lvAAAAACwEJJ9AAAAAMxXS/Ztk+wDAAAAgIWQ7AMAAABgxjpr0b4Tkn0AAAAAsBCSfQAAAADMWq+n3oLzQ7IPAAAAABZCsg8AAACA2eokbcy+E5J9AAAAALAQkn0AAAAAzFcna2P2nZDsAwAAAICFkOwDAAAAYNaM2Xdqd519q72drfrx640PKfb+2L7S3r9taL2jomNzsLU+HFrvqOgEn529wf3sExwfORzblnV4bWi9JKkrnxlec3g2fYrPTtX4moPPdaOPDwAA4OKQ7AMAAABgtjrJWrDvhDH7AAAAAGAhdPYBAAAAwEK4jBcAAACA+eqkXcd7QrIPAAAAABZCsg8AAACAWWvBvhOSfQAAAACwEJJ9AAAAAMza2ph9JyT7AAAAAGAhJPsAAAAAmK3uThu074RkHwAAAAAshGQfAAAAALPW66m34PyQ7AMAAACAhVhOsu/wYHjJ+t1HxtYbWm1jb29svSm64leD32OSGr1fa4J+/dXyf0vo2542vuYEn9fRan04vubBtcEFJzg+Rr9HAAAYaG3MvhPL/79xAAAAALgglpPsAwAAAOBCMhvvKck+AAAAAFgIyT4AAAAAZqs7Wa8l+45J9gEAAADAQkj2AQAAADBrhuw7JdkHAAAAAAuhsw8AAAAAFsJlvAAAAADMWpug44RkHwAAAAAshGQfAAAAALPV3VmboeOEZB8AAAAALIRkHwAAAACzZsy+U7vp7OtOrl3dyarPlYNrY+vt7Y2tlyQH68H1Bu/TqQw+CXUPbscJ1KXL42seHoyvObziBKaI368HHyPrw7H1kuTy7eNrAgAAw0n2AQAAADBrkn2njNkHAAAAAAsh2QcAAADAfPXw0bLONck+AAAAAFgIyT4AAAAAZqtjzL5tkn0AAAAAsBA6+wAAAACYsU73mNv1qqqXVdUHquqDVfWTW8vvq6oPVdVDVfXtW8tfWFXv3Sz/4aq6fL01dfYBAAAAwC1WVc9O8v1JXtXdX5zkP9gs30vypiSvTPLiJF9fVS/evOx7k/yN7v6iJL+T5I3XW1dnHwAAAADz1cl63UNu1+nPJfmH3f2vkqS7P75Z/tIkD3X3h7v7apK3JHl1VVWSr07yo5vn/VCS11xvUZ19AAAAAHDr/aEkd1bVu6vqp6vqGzbLn5vkN7ae95HNsruS/G53Hzxm+XUxGy8AAAAAnM3dVfX+rb/v7+77n+C5+0lekuTlSZ6W5P+tqvfsegN19gEAAAAwazcyecYNeri7732iB6vqm5N80+bPH0nyju5+JMkjVfXPkvyxHCX2Pn/rZc9L8tEkn0jy7Kra36T7jpdfF5fxAgAAAMAt0N1v6u4v7e4vTfKPkvypqtqvqqcn+fIkv5jkfUletJl593KS1yV5oI96LN+V5LWb1b0+yVuvdxt2k+zrdfLop3ey6idU4/st+57nDK13+LQ7htabwurw2tSbMERXja23Gh/irfXBUz/pFlpdvTK0XpL0r/3y8JrrT39mbMHV2M9qkqwuX/fM8jetLt82tt7e3tB6SZLLt4+vCQAAA3SSvv7JM3auu3+xqt6e5OeSrJP8ve7++SSpqm9J8o4ke0ne3N0f3Lzs25K8paq+O8nPJvn711vXZbwAAAAAsAPd/X1Jvu9xlr8tydseZ/mHczRb7w3T2QcAAADAfPX5TPZNxZh9AAAAALAQkn0AAAAAzFhnPW423nNPsg8AAAAAFkKyDwAAAIBZM2bfKck+AAAAAFgIyT4AAAAAZquTtDH7Tkj2AQAAAMBCSPYBAAAAMF+drI3Zd0KyDwAAAAAWQrIPAAAAgFkzG+8pyT4AAAAAWIjFJPv64NrwmvXIJ4fW2x9cL0lyeDi2Xq/H1kuS1d74mnuDa04xK9EEx+Ro9fRnDK+5Gl2zamy9JFlN8DvUevC5x0xhAADAjiymsw8AAACAi6jTflA/4TJeAAAAAFgIyT4AAAAAZqs76dFD85xjkn0AAAAAsBCSfQAAAADM2nptzL5jkn0AAAAAsBCSfQAAAADMmtl4T0n2AQAAAMBCSPYBAAAAMF/daWP2nZDsAwAAAICFkOwDAAAAYLY6kezbItkHAAAAAAsh2QcAAADArK17PfUmnBs76ezrw3XWn/rULlb9xDUPDofWS5LV4dia9XnPGVovSfqZTx9bb7U3tF6SpGp4yfWl24bWq/XB0HpJsrrymaH16pFPDq2XJLl6dXzNweed9ZVHh9ZLkr52bXzNwW1Z+5eG1kuSvWc+a3hNAABgPMk+AAAAAOarjdm3zZh9AAAAALAQOvsAAAAAYCFcxgsAAADAbHXaZbxbJPsAAAAAYCEk+wAAAACYtW7JvmOSfQAAAACwEJJ9AAAAAMxXJ+v1euqtODck+wAAAABgIST7AAAAAJg1s/GekuwDAAAAgIWQ7AMAAABgtjqdbmP2HZPsAwAAAICF2FGyr9OjZ0FZ1dh6SQ5//1NjC37yQ2PrJVndfvvYgvsThE0PD8fXvHp1aLn1lbH1kmT0byrd48dn2L/jmcNr1m1jj8m6fHlovSRZDX6PScb/CjjFeQcAAJaqjdm3TbIPAAAAABbCmH0AAAAAzJpk3ynJPgAAAABYCMk+AAAAAGasszYb7wnJPgAAAABYCJ19AAAAALAQLuMFAAAAYLa6TdCxTbIPAAAAABZCsg8AAACAWeu1CTqOSfYBAAAAwEJI9gEAAAAwX8bs+yySfQAAAACwEJJ9AAAAAMxYp9uYfcd20tn3K7/3OXnl2+7bxaqf0PrgcGi9JPnDL33x0Hpf+afvGVovSe6+49rQelVDyyVJDnt80f3V2JPQaoL9+tuPXBpa78Ffujq0XpK8712/MLxmDW7Mvb29ofWS5NqV8W05Rc3R3v6ND069CQAAwACSfQAAAADMVidZG7PvhDH7AAAAAGAhJPsAAAAAmK9Oem3MvmOSfQAAAACwEJJ9AAAAAMxYp43Zd0KyDwAAAAAWQrIPAAAAgFnrNmbfMck+AAAAAFgInX0AAAAAsBAu4wUAAABgvjom6Ngi2QcAAAAACyHZBwAAAMBsdTq9NkHHMck+AAAAAFiI6r711zRX1e8n+dAtXzHn0d1JHp56IxhCW18c2vri0NYXh7a+OLT1xaGtLw5tfXGcpa2f3933jNiYOamqt+do/43wcHffN6jWDdlVZ9/7u/veW75izh1tfXFo64tDW18c2vri0NYXh7a+OLT1xaGtLw5tza3iMl4AAAAAWAidfQAAAACwELvq7Lt/R+vl/NHWF4e2vji09cWhrS8ObX1xaOuLQ1tfHNr64tDW3BI7GbMPAAAAABjPZbwAAAAAsBBn6uyrqvuq6kNV9VBVffvjPH5bVf3w5vH3VtULth77js3yD1XVK866TqZxo21dVS+oqs9U1Qc2tx/Yes1LqurBzWv+dlXVwLfEEzhDW//pqvqZqjqoqtc+5rHXV9WvbG6v31qurc+hm2zrw63j+oGt5S/cnAMe2pwTLo94Lzy5M7T1f1lVv1BVP1dVP15Vz996zHE9EzfZzo7pmTlDe/+FzTH6gar6v6vqxVuP+R4+Ezfazr6Dz89Zj7+q+rNV1VV179Yyx/SM3GhbO665Jbr7SW9J9pL8yyRfkORykn+R5MWPec5fSvIDm/uvS/LDm/sv3jz/tiQv3Kxn7yzrdBt/u8m2fkGSn3+C9f5Ukq9IUkl+LMkrp36vF/12xrZ+QZI/muQfJHnt1vLPTfLhzX/v3Ny/U1ufz9vNtPXmsU89wXp/JMnrNvd/IMlfnPq9XvTbGdv6q5I8fXP/L26dwx3XM7ndTDtv/nZMz+h2xvZ+1tb9VyV5++a+7+Ezud1kO78gvoPP5nbW4y/JHUn+WZL3JLl3s8wxPaPbTba149rtpm9nSfa9NMlD3f3h7r6a5C1JXv2Y57w6yQ9t7v9okpdvephfneQt3X2lu381yUOb9Z1lnYx3M239uKrqOTn6cvKe7u4cdSa85pZvOdfrKdu6u3+tu38uyfoxr31Fknd292939+8keWeS+7T1uXUzbf24Nsf8V+foHJAcnRNec8u2mBt1lrZ+V3d/evPne5I8b3PfcT0fN9POj8sxfa6dpb0/ufXnM5IcD8jte/h83Ew7Py7n73PrrMffdyX53iSPbi1zTM/LzbT143Jccz3O0tn33CS/sfX3RzbLHvc53X2Q5PeS3PUkrz3LOhnvZto6SV5YVT9bVT9ZVf/21vM/8hTrZLybOQaf7LjW1ufPzZ5vb6+q91fVe6rqNZtldyX53c054EbWyW5cb1u/MUe/CD/Zax3X58/NtHPimJ6bM7V3VX1zVf3LJH8tyX/xFK/1Pfz8uZl2TnwHn5OnbOuq+jeTfH53/x9nfK1j+ny6mbZOHNfcpP2pN4DF+FiSf6O7P1FVL0nyj6vqi6feKOCmPb+7P1pVX5DkJ6rqwRx18jNjVfUfJ7k3yVdOvS3szhO0s2N6gbr7TUneVFV/Lsl/k+T1T/ESZugJ2tl38AWpqlWSv57kDRNvCjv2FG3tuOamnSXZ99Ekn7/19/M2yx73OVW1n+RzknziSV57lnUy3g239SZO/okk6e6fztH4BH9o8/zty4e09flwM8fgkx3X2vr8uanzbXd/dPPfDyd5d5I/nqPz+7M354DrXic7c6a2rqp/J8l3JnlVd195itc6rs+fm2lnx/T8XO85/C05vaTL9/D5uOF29h18dp6qre9I8iVJ3l1Vv5ajsdke2Ezc4Jielxtua8c1t8JZOvvel+RFdTRL2+UcTcrwwGOe80BOf0F8bZKf2FxD/kCS19XRDK4vTPKiHA0oeZZ1Mt4Nt3VV3VNVe0mySQu8KMmHu/tjST5ZVV+xGRPoG5K8dcSb4UndzDH4jiRfU1V3VtWdSb4myTu09bl1w229aePbNvfvTvJvJfmFzfn9XTk6ByRH5wRtPb2nbOuq+uNJ/qccdQB9fOshx/V83HA7O6Zn6Szt/aKtP78uya9s7vsePh833M6+g8/Ok7Z1d/9ed9/d3S/o7hfkaNzVV3X3++OYnpsbbmvHNbfEWWbxSPK1SX45Rz3K37lZ9ldz9GFMktuT/K85GiT0p5J8wdZrv3Pzug9la6aYx1un2/S3G23rJH82yQeTfCDJzyT5d7fWeW+Sn9+s8+8mqanfp9uZ2vrLcjQOxCM5Sn18cOu137j5DDyU5D/V1uf7dqNtneRPJnkwR7OHPZjkjVvr/ILNOeChzTnhtqnfp9uZ2vr/TPJbm3P1B5I8sPVax/VMbjfazo7ped7O0N5/a+s72LuSfPHWa30Pn8ntRts5voPP7vZUbf2Y5747mxlaN387pmd0u9G2dly73YpbdT/pRE4AAAAAwEyc5TJeAAAAAGAGdPYBAAAAwELo7AMAAACAhdDZBwAAAAALobMPAAAAABZCZx8AsChVdVdVfWBz+82q+ujm/qeq6vun3j4AANil6u6ptwEAYCeq6q8k+VR3/w9TbwsAAIwg2QcAXAhV9bKq+ieb+3+lqn6oqv6vqvr1qvr3q+qvVdWDVfX2qrq0ed5Lquonq+qnq+odVfWcad8FAAA8OZ19AMBF9YVJvjrJq5L8z0ne1d1/JMlnknzdpsPv7yR5bXe/JMmbk3zPVBsLAABnsT/1BgAATOTHuvtaVT2YZC/J2zfLH0zygiR/OMmXJHlnVWXznI9NsJ0AAHBmOvsAgIvqSpJ097qqrvXpQMbrHH1HqiQf7O4/MdUGAgDA9XIZLwDA4/tQknuq6k8kSVVdqqovnnibAADgSensAwB4HN19Nclrk3xvVf2LJB9I8icn3SgAAHgKdXrFCgAAAAAwZ5J9AAAAALAQOvsAAAAAYCF09gEAAADAQujsAwAAAICF0NkHAAAAAAuhsw8AAAAAFkJnHwAAAAAshM4+AAAAAFiI/x9Hb02FgeiabQAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 1800x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1800x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from librosa import display\n",
        "\n",
        "# showing how mfcc for number '1' looks like\n",
        "\n",
        "for i in range(200):\n",
        "    if int(y_train[i]) == 1:\n",
        "        plt.figure(figsize=(25, 10))\n",
        "        librosa.display.specshow(X_train[i], \n",
        "                                 x_axis=\"time\", \n",
        "                                 sr=44000)\n",
        "        plt.colorbar(format=\"%+2.f\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9Z-a-R5YLN2H"
      },
      "outputs": [],
      "source": [
        "# one shot hot\n",
        "y_train_hot = to_categorical(y_train)\n",
        "y_test_hot = to_categorical(y_test)\n",
        "y_val_hot = to_categorical(y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "386wRPxILSNW",
        "outputId": "999d8db0-3bbf-4933-99f3-c32df48da9cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 39, 39, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 39, 39, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 39, 39, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 19, 19, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 19, 19, 48)        13872     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 19, 19, 48)        192       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 19, 19, 48)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 9, 9, 48)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 9, 9, 64)          27712     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 9, 9, 64)          256       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 9, 9, 64)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 4, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               12900     \n",
            "=================================================================\n",
            "Total params: 392,020\n",
            "Trainable params: 391,476\n",
            "Non-trainable params: 544\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\REVE\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Feature dimension #\n",
        "\n",
        "channels= 1\n",
        "pool_size  = (2, 2) \n",
        "kernel_size= (3, 3)  \n",
        "\n",
        "num_classes = len(labels)\n",
        "\n",
        "\n",
        "# reshaping mfcc to feed into our model #\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], channels)\n",
        "X_test  = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], channels)\n",
        "X_val   = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2], channels)\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2], channels)\n",
        "\n",
        "keras.backend.clear_session()\n",
        "\n",
        "# model architecture #\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(Conv2D(32, kernel_size, padding=\"same\", input_shape=input_shape, kernel_regularizer=keras.regularizers.l2(l=0.1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "# 1st layer\n",
        "model.add(Conv2D(48, kernel_size, padding=\"same\", kernel_regularizer=keras.regularizers.l2(l=0.1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))  \n",
        "model.add(MaxPooling2D(pool_size=pool_size))\n",
        "    \n",
        "# 2nd layer\n",
        "model.add(Conv2D(64, kernel_size, padding=\"same\", kernel_regularizer=keras.regularizers.l2(l=0.1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))  \n",
        "model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "# 3rd layer\n",
        "model.add(Conv2D(128, kernel_size, padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))  \n",
        "\n",
        "# flatten and dense layer\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation=\"relu\"))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# output layer\n",
        "model.add(Dense(num_classes, activation=\"softmax\"))\n",
        "\n",
        "# model compile\n",
        "model.compile(loss=keras.losses.CategoricalCrossentropy(), optimizer=Adam(lr=.0001), metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZGtWhMVLTDz",
        "outputId": "2b721ca8-996a-4dd1-c417-28e3388e8a5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "113/113 [==============================] - 19s 165ms/step - loss: 0.2662 - accuracy: 0.9857 - val_loss: 1.4296 - val_accuracy: 0.7084\n",
            "Epoch 2/200\n",
            "113/113 [==============================] - 18s 160ms/step - loss: 0.2670 - accuracy: 0.9841 - val_loss: 1.8938 - val_accuracy: 0.6145\n",
            "Epoch 3/200\n",
            "113/113 [==============================] - 19s 164ms/step - loss: 0.2557 - accuracy: 0.9872 - val_loss: 1.6358 - val_accuracy: 0.6746\n",
            "Epoch 4/200\n",
            "113/113 [==============================] - 18s 159ms/step - loss: 0.2467 - accuracy: 0.9891 - val_loss: 1.3619 - val_accuracy: 0.7309\n",
            "Epoch 5/200\n",
            "113/113 [==============================] - 18s 159ms/step - loss: 0.2444 - accuracy: 0.9872 - val_loss: 1.5595 - val_accuracy: 0.6884\n",
            "Epoch 6/200\n",
            "113/113 [==============================] - 17s 151ms/step - loss: 0.2496 - accuracy: 0.9865 - val_loss: 1.5360 - val_accuracy: 0.6683\n",
            "Epoch 7/200\n",
            "113/113 [==============================] - 18s 156ms/step - loss: 0.2506 - accuracy: 0.9853 - val_loss: 1.4414 - val_accuracy: 0.6721\n",
            "Epoch 8/200\n",
            "113/113 [==============================] - 16s 146ms/step - loss: 0.2547 - accuracy: 0.9825 - val_loss: 1.9713 - val_accuracy: 0.6170\n",
            "Epoch 9/200\n",
            "113/113 [==============================] - 19s 167ms/step - loss: 0.2530 - accuracy: 0.9822 - val_loss: 1.3919 - val_accuracy: 0.6959\n",
            "Epoch 10/200\n",
            "113/113 [==============================] - 17s 149ms/step - loss: 0.2417 - accuracy: 0.9872 - val_loss: 1.5163 - val_accuracy: 0.6909\n",
            "Epoch 11/200\n",
            "113/113 [==============================] - 18s 158ms/step - loss: 0.2496 - accuracy: 0.9830 - val_loss: 1.3854 - val_accuracy: 0.7059\n",
            "Epoch 12/200\n",
            "113/113 [==============================] - 16s 141ms/step - loss: 0.2350 - accuracy: 0.9843 - val_loss: 1.5930 - val_accuracy: 0.6871\n",
            "Epoch 13/200\n",
            "113/113 [==============================] - 16s 140ms/step - loss: 0.2283 - accuracy: 0.9880 - val_loss: 1.4701 - val_accuracy: 0.7021\n",
            "Epoch 14/200\n",
            "113/113 [==============================] - 16s 143ms/step - loss: 0.2358 - accuracy: 0.9837 - val_loss: 1.5863 - val_accuracy: 0.6796\n",
            "Epoch 15/200\n",
            "113/113 [==============================] - 16s 140ms/step - loss: 0.2261 - accuracy: 0.9878 - val_loss: 1.8076 - val_accuracy: 0.6295\n",
            "Epoch 16/200\n",
            "113/113 [==============================] - 16s 141ms/step - loss: 0.2345 - accuracy: 0.9823 - val_loss: 1.3994 - val_accuracy: 0.7109\n",
            "Epoch 17/200\n",
            "113/113 [==============================] - 16s 143ms/step - loss: 0.2383 - accuracy: 0.9826 - val_loss: 1.8251 - val_accuracy: 0.6558\n",
            "Epoch 18/200\n",
            "113/113 [==============================] - 16s 144ms/step - loss: 0.2169 - accuracy: 0.9891 - val_loss: 1.6436 - val_accuracy: 0.6658\n",
            "Epoch 19/200\n",
            "113/113 [==============================] - 16s 146ms/step - loss: 0.2149 - accuracy: 0.9883 - val_loss: 1.6134 - val_accuracy: 0.6746\n",
            "Epoch 20/200\n",
            "113/113 [==============================] - 17s 147ms/step - loss: 0.2212 - accuracy: 0.9848 - val_loss: 1.7208 - val_accuracy: 0.6596\n",
            "Epoch 21/200\n",
            "113/113 [==============================] - 19s 171ms/step - loss: 0.2197 - accuracy: 0.9848 - val_loss: 1.7298 - val_accuracy: 0.6771\n",
            "Epoch 22/200\n",
            "113/113 [==============================] - 17s 147ms/step - loss: 0.2257 - accuracy: 0.9844 - val_loss: 1.7713 - val_accuracy: 0.6583\n",
            "Epoch 23/200\n",
            "113/113 [==============================] - 16s 141ms/step - loss: 0.2196 - accuracy: 0.9857 - val_loss: 1.3724 - val_accuracy: 0.7146\n",
            "Epoch 24/200\n",
            "113/113 [==============================] - 16s 138ms/step - loss: 0.2177 - accuracy: 0.9837 - val_loss: 1.3655 - val_accuracy: 0.7159\n",
            "Epoch 25/200\n",
            "113/113 [==============================] - 16s 144ms/step - loss: 0.2151 - accuracy: 0.9832 - val_loss: 1.5217 - val_accuracy: 0.6834\n",
            "Epoch 26/200\n",
            "113/113 [==============================] - 16s 143ms/step - loss: 0.1991 - accuracy: 0.9896 - val_loss: 1.7256 - val_accuracy: 0.6583\n",
            "Epoch 27/200\n",
            "113/113 [==============================] - 16s 142ms/step - loss: 0.2081 - accuracy: 0.9851 - val_loss: 1.6061 - val_accuracy: 0.6884\n",
            "Epoch 28/200\n",
            "113/113 [==============================] - 16s 143ms/step - loss: 0.2183 - accuracy: 0.9821 - val_loss: 1.5423 - val_accuracy: 0.6733\n",
            "Epoch 29/200\n",
            "113/113 [==============================] - 16s 143ms/step - loss: 0.2040 - accuracy: 0.9869 - val_loss: 1.6965 - val_accuracy: 0.6471\n",
            "Epoch 30/200\n",
            "113/113 [==============================] - 16s 139ms/step - loss: 0.2114 - accuracy: 0.9841 - val_loss: 1.4120 - val_accuracy: 0.7084\n",
            "Epoch 31/200\n",
            "113/113 [==============================] - 16s 139ms/step - loss: 0.2045 - accuracy: 0.9871 - val_loss: 1.6718 - val_accuracy: 0.6571\n",
            "Epoch 32/200\n",
            "113/113 [==============================] - 16s 145ms/step - loss: 0.1956 - accuracy: 0.9891 - val_loss: 1.4752 - val_accuracy: 0.6946\n",
            "Epoch 33/200\n",
            "113/113 [==============================] - 17s 154ms/step - loss: 0.1916 - accuracy: 0.9894 - val_loss: 1.5421 - val_accuracy: 0.6859\n",
            "Epoch 34/200\n",
            "113/113 [==============================] - 19s 168ms/step - loss: 0.2000 - accuracy: 0.9851 - val_loss: 1.4778 - val_accuracy: 0.7121\n",
            "Epoch 35/200\n",
            "113/113 [==============================] - 20s 177ms/step - loss: 0.2225 - accuracy: 0.9783 - val_loss: 1.6702 - val_accuracy: 0.6708\n",
            "Epoch 36/200\n",
            "113/113 [==============================] - 18s 157ms/step - loss: 0.1862 - accuracy: 0.9905 - val_loss: 1.8885 - val_accuracy: 0.6395\n",
            "Epoch 37/200\n",
            "113/113 [==============================] - 19s 167ms/step - loss: 0.1870 - accuracy: 0.9897 - val_loss: 1.6079 - val_accuracy: 0.6683\n",
            "Epoch 38/200\n",
            "113/113 [==============================] - 18s 159ms/step - loss: 0.2004 - accuracy: 0.9840 - val_loss: 1.5579 - val_accuracy: 0.6683\n",
            "Epoch 39/200\n",
            "113/113 [==============================] - 17s 154ms/step - loss: 0.1915 - accuracy: 0.9864 - val_loss: 1.4097 - val_accuracy: 0.6996\n",
            "Epoch 40/200\n",
            "113/113 [==============================] - 17s 149ms/step - loss: 0.1761 - accuracy: 0.9921 - val_loss: 1.4890 - val_accuracy: 0.6934\n",
            "Epoch 41/200\n",
            "113/113 [==============================] - 17s 150ms/step - loss: 0.1759 - accuracy: 0.9911 - val_loss: 1.6486 - val_accuracy: 0.6771\n",
            "Epoch 42/200\n",
            "113/113 [==============================] - 16s 141ms/step - loss: 0.1942 - accuracy: 0.9830 - val_loss: 1.8946 - val_accuracy: 0.6033\n",
            "Epoch 43/200\n",
            "113/113 [==============================] - 17s 149ms/step - loss: 0.1980 - accuracy: 0.9833 - val_loss: 1.4540 - val_accuracy: 0.6846\n",
            "Epoch 44/200\n",
            "113/113 [==============================] - 18s 157ms/step - loss: 0.1738 - accuracy: 0.9923 - val_loss: 1.3455 - val_accuracy: 0.7259\n",
            "Epoch 45/200\n",
            "113/113 [==============================] - 17s 149ms/step - loss: 0.1787 - accuracy: 0.9879 - val_loss: 1.8827 - val_accuracy: 0.6395\n",
            "Epoch 46/200\n",
            "113/113 [==============================] - 17s 153ms/step - loss: 0.1908 - accuracy: 0.9833 - val_loss: 1.6582 - val_accuracy: 0.6683\n",
            "Epoch 47/200\n",
            "113/113 [==============================] - 17s 151ms/step - loss: 0.1840 - accuracy: 0.9858 - val_loss: 1.5831 - val_accuracy: 0.6646\n",
            "Epoch 48/200\n",
            "113/113 [==============================] - 17s 150ms/step - loss: 0.1745 - accuracy: 0.9878 - val_loss: 2.0442 - val_accuracy: 0.6070\n",
            "Epoch 49/200\n",
            "113/113 [==============================] - 17s 153ms/step - loss: 0.1849 - accuracy: 0.9841 - val_loss: 1.7932 - val_accuracy: 0.6583\n",
            "Epoch 50/200\n",
            "113/113 [==============================] - 17s 148ms/step - loss: 0.1828 - accuracy: 0.9869 - val_loss: 1.6814 - val_accuracy: 0.6896\n",
            "Epoch 51/200\n",
            "113/113 [==============================] - 17s 150ms/step - loss: 0.1854 - accuracy: 0.9843 - val_loss: 1.4102 - val_accuracy: 0.7096\n",
            "Epoch 52/200\n",
            "113/113 [==============================] - 17s 151ms/step - loss: 0.1708 - accuracy: 0.9893 - val_loss: 1.7366 - val_accuracy: 0.6583\n",
            "Epoch 53/200\n",
            "113/113 [==============================] - 17s 154ms/step - loss: 0.1657 - accuracy: 0.9911 - val_loss: 1.6742 - val_accuracy: 0.6884\n",
            "Epoch 54/200\n",
            "113/113 [==============================] - 17s 150ms/step - loss: 0.1636 - accuracy: 0.9900 - val_loss: 1.4986 - val_accuracy: 0.7184\n",
            "Epoch 55/200\n",
            "113/113 [==============================] - 18s 157ms/step - loss: 0.1695 - accuracy: 0.9875 - val_loss: 1.8661 - val_accuracy: 0.6458\n",
            "Epoch 56/200\n",
            "113/113 [==============================] - 19s 169ms/step - loss: 0.1714 - accuracy: 0.9871 - val_loss: 1.3513 - val_accuracy: 0.7222\n",
            "Epoch 57/200\n",
            "113/113 [==============================] - 20s 173ms/step - loss: 0.1899 - accuracy: 0.9797 - val_loss: 1.8153 - val_accuracy: 0.6508\n",
            "Epoch 58/200\n",
            "113/113 [==============================] - 17s 147ms/step - loss: 0.1877 - accuracy: 0.9821 - val_loss: 1.7762 - val_accuracy: 0.6583\n",
            "Epoch 59/200\n",
            "113/113 [==============================] - 16s 141ms/step - loss: 0.1720 - accuracy: 0.9876 - val_loss: 1.3372 - val_accuracy: 0.7234\n",
            "Epoch 60/200\n",
            "113/113 [==============================] - 16s 144ms/step - loss: 0.1710 - accuracy: 0.9882 - val_loss: 1.9585 - val_accuracy: 0.6258\n",
            "Epoch 61/200\n",
            "113/113 [==============================] - 16s 142ms/step - loss: 0.1574 - accuracy: 0.9936 - val_loss: 1.3141 - val_accuracy: 0.7146\n",
            "Epoch 62/200\n",
            "113/113 [==============================] - 16s 141ms/step - loss: 0.1699 - accuracy: 0.9869 - val_loss: 1.2971 - val_accuracy: 0.7447\n",
            "Epoch 63/200\n",
            "113/113 [==============================] - 16s 144ms/step - loss: 0.1693 - accuracy: 0.9865 - val_loss: 1.4037 - val_accuracy: 0.7247\n",
            "Epoch 64/200\n",
            "113/113 [==============================] - 16s 142ms/step - loss: 0.1661 - accuracy: 0.9872 - val_loss: 1.3733 - val_accuracy: 0.7234\n",
            "Epoch 65/200\n",
            "113/113 [==============================] - 16s 139ms/step - loss: 0.1637 - accuracy: 0.9896 - val_loss: 1.4094 - val_accuracy: 0.7071\n",
            "Epoch 66/200\n",
            "113/113 [==============================] - 16s 140ms/step - loss: 0.1599 - accuracy: 0.9911 - val_loss: 1.9206 - val_accuracy: 0.6358\n",
            "Epoch 67/200\n",
            "113/113 [==============================] - 16s 144ms/step - loss: 0.1755 - accuracy: 0.9846 - val_loss: 1.8120 - val_accuracy: 0.6658\n",
            "Epoch 68/200\n",
            "113/113 [==============================] - 16s 139ms/step - loss: 0.1601 - accuracy: 0.9889 - val_loss: 1.6636 - val_accuracy: 0.6671\n",
            "Epoch 69/200\n",
            "113/113 [==============================] - 16s 142ms/step - loss: 0.1728 - accuracy: 0.9854 - val_loss: 1.4598 - val_accuracy: 0.7272\n",
            "Epoch 70/200\n",
            "113/113 [==============================] - 16s 143ms/step - loss: 0.1498 - accuracy: 0.9917 - val_loss: 1.9441 - val_accuracy: 0.6370\n",
            "Epoch 71/200\n",
            "113/113 [==============================] - 16s 146ms/step - loss: 0.1705 - accuracy: 0.9844 - val_loss: 1.5126 - val_accuracy: 0.7071\n",
            "Epoch 72/200\n",
            "113/113 [==============================] - 16s 141ms/step - loss: 0.1640 - accuracy: 0.9869 - val_loss: 1.6684 - val_accuracy: 0.6683\n",
            "Epoch 73/200\n",
            "113/113 [==============================] - 16s 141ms/step - loss: 0.1483 - accuracy: 0.9926 - val_loss: 1.3224 - val_accuracy: 0.7334\n",
            "Epoch 74/200\n",
            "113/113 [==============================] - 16s 139ms/step - loss: 0.1535 - accuracy: 0.9893 - val_loss: 1.4301 - val_accuracy: 0.7334\n",
            "Epoch 75/200\n",
            "113/113 [==============================] - 16s 145ms/step - loss: 0.1570 - accuracy: 0.9876 - val_loss: 1.3312 - val_accuracy: 0.7234\n",
            "Epoch 76/200\n",
            "113/113 [==============================] - 16s 140ms/step - loss: 0.1468 - accuracy: 0.9917 - val_loss: 1.7283 - val_accuracy: 0.6758\n",
            "Epoch 77/200\n",
            "113/113 [==============================] - 16s 140ms/step - loss: 0.1586 - accuracy: 0.9878 - val_loss: 1.5180 - val_accuracy: 0.6959\n",
            "Epoch 78/200\n",
            "113/113 [==============================] - 16s 145ms/step - loss: 0.1594 - accuracy: 0.9872 - val_loss: 1.4572 - val_accuracy: 0.7121\n",
            "Epoch 79/200\n",
            "113/113 [==============================] - 16s 141ms/step - loss: 0.1591 - accuracy: 0.9859 - val_loss: 1.5758 - val_accuracy: 0.6796\n",
            "Epoch 80/200\n",
            "113/113 [==============================] - 16s 141ms/step - loss: 0.1726 - accuracy: 0.9829 - val_loss: 1.6076 - val_accuracy: 0.7146\n",
            "Epoch 81/200\n",
            "113/113 [==============================] - 16s 140ms/step - loss: 0.1502 - accuracy: 0.9908 - val_loss: 1.6021 - val_accuracy: 0.6821\n",
            "Epoch 82/200\n",
            "113/113 [==============================] - 16s 143ms/step - loss: 0.1514 - accuracy: 0.9900 - val_loss: 1.3230 - val_accuracy: 0.7534\n",
            "Epoch 83/200\n",
            "113/113 [==============================] - 16s 141ms/step - loss: 0.1544 - accuracy: 0.9882 - val_loss: 1.7675 - val_accuracy: 0.6683\n",
            "Epoch 84/200\n",
            "113/113 [==============================] - 18s 163ms/step - loss: 0.1635 - accuracy: 0.9859 - val_loss: 1.7184 - val_accuracy: 0.6733\n",
            "Epoch 85/200\n",
            "113/113 [==============================] - 17s 146ms/step - loss: 0.1480 - accuracy: 0.9908 - val_loss: 1.2594 - val_accuracy: 0.7196\n",
            "Epoch 86/200\n",
            "113/113 [==============================] - 17s 155ms/step - loss: 0.1400 - accuracy: 0.9933 - val_loss: 1.4698 - val_accuracy: 0.6996\n",
            "Epoch 87/200\n",
            "113/113 [==============================] - 16s 141ms/step - loss: 0.1465 - accuracy: 0.9890 - val_loss: 1.4700 - val_accuracy: 0.6984\n",
            "Epoch 88/200\n",
            "113/113 [==============================] - 16s 140ms/step - loss: 0.1476 - accuracy: 0.9893 - val_loss: 1.4772 - val_accuracy: 0.7196\n",
            "Epoch 89/200\n",
            "113/113 [==============================] - 16s 145ms/step - loss: 0.1719 - accuracy: 0.9812 - val_loss: 1.9220 - val_accuracy: 0.6295\n",
            "Epoch 90/200\n",
            "113/113 [==============================] - 16s 140ms/step - loss: 0.1621 - accuracy: 0.9862 - val_loss: 1.4915 - val_accuracy: 0.7146\n",
            "Epoch 91/200\n",
            "113/113 [==============================] - 16s 140ms/step - loss: 0.1437 - accuracy: 0.9919 - val_loss: 1.3739 - val_accuracy: 0.7134\n",
            "Epoch 92/200\n",
            "113/113 [==============================] - 16s 141ms/step - loss: 0.1448 - accuracy: 0.9901 - val_loss: 1.5119 - val_accuracy: 0.7046\n",
            "Epoch 93/200\n",
            "113/113 [==============================] - 16s 144ms/step - loss: 0.1349 - accuracy: 0.9937 - val_loss: 1.3131 - val_accuracy: 0.7447\n",
            "Epoch 94/200\n",
            "113/113 [==============================] - 16s 139ms/step - loss: 0.1340 - accuracy: 0.9919 - val_loss: 1.3080 - val_accuracy: 0.7534\n",
            "Epoch 95/200\n",
            "113/113 [==============================] - 16s 141ms/step - loss: 0.1304 - accuracy: 0.9939 - val_loss: 1.7958 - val_accuracy: 0.6671\n",
            "Epoch 96/200\n",
            "113/113 [==============================] - 16s 142ms/step - loss: 0.1609 - accuracy: 0.9821 - val_loss: 1.8016 - val_accuracy: 0.6546\n",
            "Epoch 97/200\n",
            "113/113 [==============================] - 16s 144ms/step - loss: 0.1503 - accuracy: 0.9869 - val_loss: 1.4042 - val_accuracy: 0.7084\n",
            "Epoch 98/200\n",
            "113/113 [==============================] - 16s 140ms/step - loss: 0.1462 - accuracy: 0.9883 - val_loss: 1.8671 - val_accuracy: 0.6558\n",
            "Epoch 99/200\n",
            "113/113 [==============================] - 16s 138ms/step - loss: 0.1432 - accuracy: 0.9889 - val_loss: 1.7759 - val_accuracy: 0.6571\n",
            "Epoch 100/200\n",
            "113/113 [==============================] - 15s 135ms/step - loss: 0.1422 - accuracy: 0.9894 - val_loss: 1.4179 - val_accuracy: 0.7171\n",
            "Epoch 101/200\n",
            "113/113 [==============================] - 16s 140ms/step - loss: 0.1476 - accuracy: 0.9866 - val_loss: 1.5809 - val_accuracy: 0.6834\n",
            "Epoch 102/200\n",
            "113/113 [==============================] - 15s 134ms/step - loss: 0.1435 - accuracy: 0.9897 - val_loss: 1.6564 - val_accuracy: 0.6821\n",
            "Epoch 103/200\n",
            "113/113 [==============================] - 15s 134ms/step - loss: 0.1625 - accuracy: 0.9821 - val_loss: 1.5005 - val_accuracy: 0.7109\n",
            "Epoch 104/200\n",
            "113/113 [==============================] - 15s 134ms/step - loss: 0.1479 - accuracy: 0.9879 - val_loss: 1.5539 - val_accuracy: 0.7121\n",
            "Epoch 105/200\n",
            "113/113 [==============================] - 16s 139ms/step - loss: 0.1413 - accuracy: 0.9897 - val_loss: 1.5667 - val_accuracy: 0.7021\n",
            "Epoch 106/200\n",
            "113/113 [==============================] - 15s 133ms/step - loss: 0.1463 - accuracy: 0.9886 - val_loss: 1.5471 - val_accuracy: 0.6921\n",
            "Epoch 107/200\n",
            "113/113 [==============================] - 15s 134ms/step - loss: 0.1309 - accuracy: 0.9936 - val_loss: 1.4783 - val_accuracy: 0.7259\n",
            "Epoch 108/200\n",
            "113/113 [==============================] - 15s 135ms/step - loss: 0.1216 - accuracy: 0.9946 - val_loss: 1.3577 - val_accuracy: 0.7322\n",
            "Epoch 109/200\n",
            "113/113 [==============================] - 15s 137ms/step - loss: 0.1259 - accuracy: 0.9929 - val_loss: 1.6128 - val_accuracy: 0.6796\n",
            "Epoch 110/200\n",
            "113/113 [==============================] - 15s 135ms/step - loss: 0.1450 - accuracy: 0.9876 - val_loss: 1.8108 - val_accuracy: 0.6596\n",
            "Epoch 111/200\n",
            "113/113 [==============================] - 15s 134ms/step - loss: 0.1410 - accuracy: 0.9885 - val_loss: 1.4931 - val_accuracy: 0.7121\n",
            "Epoch 112/200\n",
            "113/113 [==============================] - 15s 135ms/step - loss: 0.1425 - accuracy: 0.9876 - val_loss: 1.6814 - val_accuracy: 0.6871\n",
            "Epoch 113/200\n",
            "113/113 [==============================] - 15s 135ms/step - loss: 0.1456 - accuracy: 0.9869 - val_loss: 1.6108 - val_accuracy: 0.6996\n",
            "Epoch 114/200\n",
            "113/113 [==============================] - 16s 138ms/step - loss: 0.1372 - accuracy: 0.9908 - val_loss: 1.6290 - val_accuracy: 0.7134\n",
            "Epoch 115/200\n",
            "113/113 [==============================] - 15s 133ms/step - loss: 0.1399 - accuracy: 0.9885 - val_loss: 1.3131 - val_accuracy: 0.7622\n",
            "Epoch 116/200\n",
            "113/113 [==============================] - 15s 136ms/step - loss: 0.1270 - accuracy: 0.9929 - val_loss: 1.7078 - val_accuracy: 0.6946\n",
            "Epoch 117/200\n",
            "113/113 [==============================] - 15s 135ms/step - loss: 0.1213 - accuracy: 0.9943 - val_loss: 1.5988 - val_accuracy: 0.7021\n",
            "Epoch 118/200\n",
            "113/113 [==============================] - 15s 135ms/step - loss: 0.1275 - accuracy: 0.9915 - val_loss: 1.4169 - val_accuracy: 0.7247\n",
            "Epoch 119/200\n",
            "113/113 [==============================] - 15s 135ms/step - loss: 0.1512 - accuracy: 0.9840 - val_loss: 1.6226 - val_accuracy: 0.6984\n",
            "Epoch 120/200\n",
            "113/113 [==============================] - 15s 137ms/step - loss: 0.1560 - accuracy: 0.9823 - val_loss: 1.6175 - val_accuracy: 0.6746\n",
            "Epoch 121/200\n",
            "113/113 [==============================] - 15s 134ms/step - loss: 0.1306 - accuracy: 0.9923 - val_loss: 1.4949 - val_accuracy: 0.7247\n",
            "Epoch 122/200\n",
            "113/113 [==============================] - 16s 144ms/step - loss: 0.1224 - accuracy: 0.9936 - val_loss: 1.3009 - val_accuracy: 0.7547\n",
            "Epoch 123/200\n",
            "113/113 [==============================] - 19s 165ms/step - loss: 0.1361 - accuracy: 0.9897 - val_loss: 1.4700 - val_accuracy: 0.7084\n",
            "Epoch 124/200\n",
            "113/113 [==============================] - 18s 164ms/step - loss: 0.1422 - accuracy: 0.9883 - val_loss: 1.4145 - val_accuracy: 0.7247\n",
            "Epoch 125/200\n",
            "113/113 [==============================] - 17s 155ms/step - loss: 0.1361 - accuracy: 0.9894 - val_loss: 1.6027 - val_accuracy: 0.6821\n",
            "Epoch 126/200\n",
            "113/113 [==============================] - 19s 166ms/step - loss: 0.1288 - accuracy: 0.9915 - val_loss: 1.3888 - val_accuracy: 0.7259\n",
            "Epoch 127/200\n",
            "113/113 [==============================] - 19s 171ms/step - loss: 0.1300 - accuracy: 0.9907 - val_loss: 1.8147 - val_accuracy: 0.6546\n",
            "Epoch 128/200\n",
            "113/113 [==============================] - 16s 140ms/step - loss: 0.1297 - accuracy: 0.9901 - val_loss: 1.5134 - val_accuracy: 0.6984\n",
            "Epoch 129/200\n",
            "113/113 [==============================] - 16s 139ms/step - loss: 0.1360 - accuracy: 0.9887 - val_loss: 1.4415 - val_accuracy: 0.7171\n",
            "Epoch 130/200\n",
            "113/113 [==============================] - 16s 141ms/step - loss: 0.1290 - accuracy: 0.9908 - val_loss: 1.4506 - val_accuracy: 0.7184\n",
            "Epoch 131/200\n",
            "113/113 [==============================] - 17s 150ms/step - loss: 0.1189 - accuracy: 0.9942 - val_loss: 1.3761 - val_accuracy: 0.7622\n",
            "Epoch 132/200\n",
            "113/113 [==============================] - 17s 152ms/step - loss: 0.1100 - accuracy: 0.9961 - val_loss: 1.3788 - val_accuracy: 0.7497\n",
            "Epoch 133/200\n",
            "113/113 [==============================] - 16s 142ms/step - loss: 0.1390 - accuracy: 0.9855 - val_loss: 1.6095 - val_accuracy: 0.7009\n",
            "Epoch 134/200\n",
            "113/113 [==============================] - 18s 157ms/step - loss: 0.1480 - accuracy: 0.9841 - val_loss: 1.2216 - val_accuracy: 0.7597\n",
            "Epoch 135/200\n",
            "113/113 [==============================] - 19s 170ms/step - loss: 0.1485 - accuracy: 0.9854 - val_loss: 1.4621 - val_accuracy: 0.7159\n",
            "Epoch 136/200\n",
            "113/113 [==============================] - 18s 160ms/step - loss: 0.1520 - accuracy: 0.9827 - val_loss: 1.6703 - val_accuracy: 0.6646\n",
            "Epoch 137/200\n",
            "113/113 [==============================] - 17s 149ms/step - loss: 0.1344 - accuracy: 0.9910 - val_loss: 1.4785 - val_accuracy: 0.7297\n",
            "Epoch 138/200\n",
            "113/113 [==============================] - 17s 147ms/step - loss: 0.1244 - accuracy: 0.9926 - val_loss: 1.4392 - val_accuracy: 0.7297\n",
            "Epoch 139/200\n",
            "113/113 [==============================] - 17s 151ms/step - loss: 0.1196 - accuracy: 0.9933 - val_loss: 1.4886 - val_accuracy: 0.7297\n",
            "Epoch 140/200\n",
            "113/113 [==============================] - 17s 148ms/step - loss: 0.1318 - accuracy: 0.9901 - val_loss: 1.3847 - val_accuracy: 0.7334\n",
            "Epoch 141/200\n",
            "113/113 [==============================] - 18s 159ms/step - loss: 0.1147 - accuracy: 0.9949 - val_loss: 1.5590 - val_accuracy: 0.7034\n",
            "Epoch 142/200\n",
            "113/113 [==============================] - 17s 151ms/step - loss: 0.1214 - accuracy: 0.9911 - val_loss: 1.8078 - val_accuracy: 0.6846\n",
            "Epoch 143/200\n",
            "113/113 [==============================] - 17s 151ms/step - loss: 0.1320 - accuracy: 0.9872 - val_loss: 2.1426 - val_accuracy: 0.6183\n",
            "Epoch 144/200\n",
            "113/113 [==============================] - 17s 152ms/step - loss: 0.1319 - accuracy: 0.9893 - val_loss: 1.6402 - val_accuracy: 0.6934\n",
            "Epoch 145/200\n",
            "113/113 [==============================] - 18s 161ms/step - loss: 0.1225 - accuracy: 0.9919 - val_loss: 1.8311 - val_accuracy: 0.6959\n",
            "Epoch 146/200\n",
            "113/113 [==============================] - 16s 144ms/step - loss: 0.1359 - accuracy: 0.9879 - val_loss: 1.3782 - val_accuracy: 0.7109\n",
            "Epoch 147/200\n",
            "113/113 [==============================] - 17s 153ms/step - loss: 0.1209 - accuracy: 0.9923 - val_loss: 1.7618 - val_accuracy: 0.6796\n",
            "Epoch 148/200\n",
            "113/113 [==============================] - 19s 164ms/step - loss: 0.1203 - accuracy: 0.9921 - val_loss: 1.4331 - val_accuracy: 0.7359\n",
            "Epoch 149/200\n",
            "113/113 [==============================] - 17s 154ms/step - loss: 0.1189 - accuracy: 0.9910 - val_loss: 1.9954 - val_accuracy: 0.6308\n",
            "Epoch 150/200\n",
            "113/113 [==============================] - 18s 162ms/step - loss: 0.1518 - accuracy: 0.9822 - val_loss: 2.1049 - val_accuracy: 0.5707\n",
            "Epoch 151/200\n",
            "113/113 [==============================] - 17s 149ms/step - loss: 0.1607 - accuracy: 0.9801 - val_loss: 1.2594 - val_accuracy: 0.7409\n",
            "Epoch 152/200\n",
            "113/113 [==============================] - 18s 163ms/step - loss: 0.1294 - accuracy: 0.9905 - val_loss: 1.4710 - val_accuracy: 0.7472\n",
            "Epoch 153/200\n",
            "113/113 [==============================] - 16s 143ms/step - loss: 0.1171 - accuracy: 0.9947 - val_loss: 1.3738 - val_accuracy: 0.7472\n",
            "Epoch 154/200\n",
            "113/113 [==============================] - 16s 143ms/step - loss: 0.1136 - accuracy: 0.9953 - val_loss: 1.4996 - val_accuracy: 0.7359\n",
            "Epoch 155/200\n",
            "113/113 [==============================] - 19s 169ms/step - loss: 0.1103 - accuracy: 0.9955 - val_loss: 1.3376 - val_accuracy: 0.7322\n",
            "Epoch 156/200\n",
            "113/113 [==============================] - 19s 164ms/step - loss: 0.1202 - accuracy: 0.9903 - val_loss: 1.5653 - val_accuracy: 0.7234\n",
            "Epoch 157/200\n",
            "113/113 [==============================] - 19s 168ms/step - loss: 0.1162 - accuracy: 0.9926 - val_loss: 1.5990 - val_accuracy: 0.7059\n",
            "Epoch 158/200\n",
            "113/113 [==============================] - 18s 157ms/step - loss: 0.1417 - accuracy: 0.9854 - val_loss: 1.3916 - val_accuracy: 0.7272\n",
            "Epoch 159/200\n",
            "113/113 [==============================] - 17s 148ms/step - loss: 0.1283 - accuracy: 0.9900 - val_loss: 2.1574 - val_accuracy: 0.6358\n",
            "Epoch 160/200\n",
            "113/113 [==============================] - 17s 146ms/step - loss: 0.1511 - accuracy: 0.9819 - val_loss: 1.6438 - val_accuracy: 0.7171\n",
            "Epoch 161/200\n",
            "113/113 [==============================] - 16s 144ms/step - loss: 0.1244 - accuracy: 0.9918 - val_loss: 1.4920 - val_accuracy: 0.7046\n",
            "Epoch 162/200\n",
            "113/113 [==============================] - 16s 142ms/step - loss: 0.1173 - accuracy: 0.9936 - val_loss: 1.2299 - val_accuracy: 0.7484\n",
            "Epoch 163/200\n",
            "113/113 [==============================] - 17s 150ms/step - loss: 0.1061 - accuracy: 0.9962 - val_loss: 1.3290 - val_accuracy: 0.7447\n",
            "Epoch 164/200\n",
            "113/113 [==============================] - 15s 134ms/step - loss: 0.1040 - accuracy: 0.9965 - val_loss: 1.2838 - val_accuracy: 0.7772\n",
            "Epoch 165/200\n",
            "113/113 [==============================] - 17s 153ms/step - loss: 0.1256 - accuracy: 0.9878 - val_loss: 1.4892 - val_accuracy: 0.7046\n",
            "Epoch 166/200\n",
            "113/113 [==============================] - 18s 159ms/step - loss: 0.1315 - accuracy: 0.9871 - val_loss: 1.7318 - val_accuracy: 0.7159\n",
            "Epoch 167/200\n",
            "113/113 [==============================] - 18s 157ms/step - loss: 0.1206 - accuracy: 0.9914 - val_loss: 1.2330 - val_accuracy: 0.7584\n",
            "Epoch 168/200\n",
            "113/113 [==============================] - 16s 143ms/step - loss: 0.1155 - accuracy: 0.9929 - val_loss: 1.3260 - val_accuracy: 0.7447\n",
            "Epoch 169/200\n",
            "113/113 [==============================] - 16s 143ms/step - loss: 0.1085 - accuracy: 0.9946 - val_loss: 2.1490 - val_accuracy: 0.6496\n",
            "Epoch 170/200\n",
            "113/113 [==============================] - 16s 141ms/step - loss: 0.1442 - accuracy: 0.9822 - val_loss: 1.8391 - val_accuracy: 0.6646\n",
            "Epoch 171/200\n",
            "113/113 [==============================] - 16s 141ms/step - loss: 0.1373 - accuracy: 0.9868 - val_loss: 1.5756 - val_accuracy: 0.7071\n",
            "Epoch 172/200\n",
            "113/113 [==============================] - 16s 142ms/step - loss: 0.1273 - accuracy: 0.9897 - val_loss: 1.3470 - val_accuracy: 0.7447\n",
            "Epoch 173/200\n",
            "113/113 [==============================] - 16s 144ms/step - loss: 0.1081 - accuracy: 0.9953 - val_loss: 1.4723 - val_accuracy: 0.7422\n",
            "Epoch 174/200\n",
            "113/113 [==============================] - 16s 142ms/step - loss: 0.1103 - accuracy: 0.9926 - val_loss: 1.4708 - val_accuracy: 0.7222\n",
            "Epoch 175/200\n",
            "113/113 [==============================] - 17s 149ms/step - loss: 0.1086 - accuracy: 0.9928 - val_loss: 1.7826 - val_accuracy: 0.6834\n",
            "Epoch 176/200\n",
            "113/113 [==============================] - 19s 169ms/step - loss: 0.1303 - accuracy: 0.9857 - val_loss: 2.4623 - val_accuracy: 0.5945\n",
            "Epoch 177/200\n",
            "113/113 [==============================] - 16s 145ms/step - loss: 0.1185 - accuracy: 0.9912 - val_loss: 1.6640 - val_accuracy: 0.7046\n",
            "Epoch 178/200\n",
            "113/113 [==============================] - 16s 144ms/step - loss: 0.1186 - accuracy: 0.9911 - val_loss: 2.1009 - val_accuracy: 0.6383\n",
            "Epoch 179/200\n",
            "113/113 [==============================] - 18s 157ms/step - loss: 0.1313 - accuracy: 0.9850 - val_loss: 1.9377 - val_accuracy: 0.6721\n",
            "Epoch 180/200\n",
            "113/113 [==============================] - 20s 179ms/step - loss: 0.1276 - accuracy: 0.9883 - val_loss: 1.8572 - val_accuracy: 0.6796\n",
            "Epoch 181/200\n",
            "113/113 [==============================] - 19s 165ms/step - loss: 0.1195 - accuracy: 0.9912 - val_loss: 1.2881 - val_accuracy: 0.7384\n",
            "Epoch 182/200\n",
            "113/113 [==============================] - 17s 149ms/step - loss: 0.1185 - accuracy: 0.9910 - val_loss: 1.5634 - val_accuracy: 0.7184\n",
            "Epoch 183/200\n",
            "113/113 [==============================] - 17s 147ms/step - loss: 0.1284 - accuracy: 0.9880 - val_loss: 1.8805 - val_accuracy: 0.6809\n",
            "Epoch 184/200\n",
            "113/113 [==============================] - 16s 141ms/step - loss: 0.1165 - accuracy: 0.9921 - val_loss: 1.4450 - val_accuracy: 0.7309\n",
            "Epoch 185/200\n",
            "113/113 [==============================] - 16s 142ms/step - loss: 0.0991 - accuracy: 0.9961 - val_loss: 1.5554 - val_accuracy: 0.7171\n",
            "Epoch 186/200\n",
            "113/113 [==============================] - 16s 142ms/step - loss: 0.0944 - accuracy: 0.9981 - val_loss: 1.4573 - val_accuracy: 0.7409\n",
            "Epoch 187/200\n",
            "113/113 [==============================] - 16s 144ms/step - loss: 0.0935 - accuracy: 0.9964 - val_loss: 2.0670 - val_accuracy: 0.6446\n",
            "Epoch 188/200\n",
            "113/113 [==============================] - 16s 142ms/step - loss: 0.1010 - accuracy: 0.9937 - val_loss: 1.7034 - val_accuracy: 0.6696\n",
            "Epoch 189/200\n",
            "113/113 [==============================] - 16s 140ms/step - loss: 0.1239 - accuracy: 0.9859 - val_loss: 2.0377 - val_accuracy: 0.6733\n",
            "Epoch 190/200\n",
            "113/113 [==============================] - 16s 141ms/step - loss: 0.1351 - accuracy: 0.9848 - val_loss: 1.9424 - val_accuracy: 0.6370\n",
            "Epoch 191/200\n",
            "113/113 [==============================] - 16s 144ms/step - loss: 0.1369 - accuracy: 0.9829 - val_loss: 2.0102 - val_accuracy: 0.6521\n",
            "Epoch 192/200\n",
            "113/113 [==============================] - 16s 139ms/step - loss: 0.1346 - accuracy: 0.9869 - val_loss: 1.5287 - val_accuracy: 0.7359\n",
            "Epoch 193/200\n",
            "113/113 [==============================] - 16s 141ms/step - loss: 0.1112 - accuracy: 0.9942 - val_loss: 1.3588 - val_accuracy: 0.7597\n",
            "Epoch 194/200\n",
            "113/113 [==============================] - 16s 145ms/step - loss: 0.1051 - accuracy: 0.9951 - val_loss: 1.3899 - val_accuracy: 0.7347\n",
            "Epoch 195/200\n",
            "113/113 [==============================] - 15s 136ms/step - loss: 0.1004 - accuracy: 0.9961 - val_loss: 1.4110 - val_accuracy: 0.7572\n",
            "Epoch 196/200\n",
            "113/113 [==============================] - 15s 134ms/step - loss: 0.1056 - accuracy: 0.9937 - val_loss: 1.5824 - val_accuracy: 0.7159\n",
            "Epoch 197/200\n",
            "113/113 [==============================] - 15s 136ms/step - loss: 0.1352 - accuracy: 0.9826 - val_loss: 1.8223 - val_accuracy: 0.6658\n",
            "Epoch 198/200\n",
            "113/113 [==============================] - 17s 148ms/step - loss: 0.1360 - accuracy: 0.9850 - val_loss: 1.4271 - val_accuracy: 0.7672\n",
            "Epoch 199/200\n",
            "113/113 [==============================] - 16s 138ms/step - loss: 0.1161 - accuracy: 0.9918 - val_loss: 1.4170 - val_accuracy: 0.7622\n",
            "Epoch 200/200\n",
            "113/113 [==============================] - 19s 167ms/step - loss: 0.1037 - accuracy: 0.9949 - val_loss: 1.4599 - val_accuracy: 0.7484\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x225064d6970>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_train, y_train_hot, epochs=200, batch_size=64, validation_data=(X_val, y_val_hot))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shUvWQqMLYOH",
        "outputId": "b287cf56-dd2b-4653-d5eb-8080d4f0842a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 1s 21ms/step - loss: 1.2332 - accuracy: 0.7782\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[1.2332206964492798, 0.7781672477722168]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(X_test, y_test_hot, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GJoecCC-LZ0H"
      },
      "outputs": [],
      "source": [
        "model.save('%_on_100_class_CNN.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuSTLvMXTpf8",
        "outputId": "77e5a377-002f-44a9-87ff-752ac9a7eebf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision, Recall, F1-score on test dataset\n",
            "precision:  0.7781672508763144  recall:  0.7781672508763144 f1 score : 0.7781672508763144\n",
            "Precision, Recall, F1-score on each class of test dataset\n",
            "0 :  89.47368421052632   precision :  0.7391304347826086  recall :  0.8947368421052632  f1score:  0.8095238095238095\n",
            "1 :  100.0   precision :  0.9166666666666666  recall :  1.0  f1score:  0.9565217391304348\n",
            "2 :  90.0   precision :  0.75  recall :  0.9  f1score:  0.8181818181818182\n",
            "3 :  92.3076923076923   precision :  0.7058823529411765  recall :  0.9230769230769231  f1score:  0.8000000000000002\n",
            "4 :  76.47058823529412   precision :  0.9285714285714286  recall :  0.7647058823529411  f1score:  0.8387096774193549\n",
            "5 :  86.66666666666667   precision :  0.6842105263157895  recall :  0.8666666666666667  f1score:  0.7647058823529413\n",
            "6 :  83.33333333333334   precision :  0.9523809523809523  recall :  0.8333333333333334  f1score:  0.888888888888889\n",
            "7 :  75.0   precision :  0.5454545454545454  recall :  0.75  f1score:  0.631578947368421\n",
            "8 :  73.07692307692307   precision :  0.7916666666666666  recall :  0.7307692307692307  f1score:  0.76\n",
            "9 :  77.77777777777779   precision :  0.7368421052631579  recall :  0.7777777777777778  f1score:  0.7567567567567567\n",
            "10 :  61.111111111111114   precision :  1.0  recall :  0.6111111111111112  f1score:  0.7586206896551725\n",
            "11 :  100.0   precision :  0.9545454545454546  recall :  1.0  f1score:  0.9767441860465117\n",
            "12 :  66.66666666666666   precision :  1.0  recall :  0.6666666666666666  f1score:  0.8\n",
            "13 :  76.47058823529412   precision :  1.0  recall :  0.7647058823529411  f1score:  0.8666666666666666\n",
            "14 :  82.35294117647058   precision :  1.0  recall :  0.8235294117647058  f1score:  0.9032258064516129\n",
            "15 :  86.36363636363636   precision :  0.8260869565217391  recall :  0.8636363636363636  f1score:  0.8444444444444444\n",
            "16 :  71.42857142857143   precision :  0.8333333333333334  recall :  0.7142857142857143  f1score:  0.7692307692307692\n",
            "17 :  78.94736842105263   precision :  0.9375  recall :  0.7894736842105263  f1score:  0.8571428571428572\n",
            "18 :  84.61538461538461   precision :  0.6875  recall :  0.8461538461538461  f1score:  0.7586206896551724\n",
            "19 :  80.76923076923077   precision :  0.7777777777777778  recall :  0.8076923076923077  f1score:  0.7924528301886792\n",
            "20 :  81.25   precision :  0.43333333333333335  recall :  0.8125  f1score:  0.5652173913043479\n",
            "21 :  88.88888888888889   precision :  1.0  recall :  0.8888888888888888  f1score:  0.9411764705882353\n",
            "22 :  83.33333333333334   precision :  0.8333333333333334  recall :  0.8333333333333334  f1score:  0.8333333333333334\n",
            "23 :  61.111111111111114   precision :  0.7333333333333333  recall :  0.6111111111111112  f1score:  0.6666666666666666\n",
            "24 :  78.94736842105263   precision :  0.8333333333333334  recall :  0.7894736842105263  f1score:  0.8108108108108109\n",
            "25 :  72.72727272727273   precision :  0.9411764705882353  recall :  0.7272727272727273  f1score:  0.8205128205128205\n",
            "26 :  80.0   precision :  1.0  recall :  0.8  f1score:  0.888888888888889\n",
            "27 :  78.94736842105263   precision :  0.9375  recall :  0.7894736842105263  f1score:  0.8571428571428572\n",
            "28 :  86.36363636363636   precision :  0.8260869565217391  recall :  0.8636363636363636  f1score:  0.8444444444444444\n",
            "29 :  77.77777777777779   precision :  0.7  recall :  0.7777777777777778  f1score:  0.7368421052631577\n",
            "30 :  58.333333333333336   precision :  0.6363636363636364  recall :  0.5833333333333334  f1score:  0.6086956521739131\n",
            "31 :  35.294117647058826   precision :  0.4  recall :  0.35294117647058826  f1score:  0.37500000000000006\n",
            "32 :  88.88888888888889   precision :  0.6666666666666666  recall :  0.8888888888888888  f1score:  0.761904761904762\n",
            "33 :  80.0   precision :  0.64  recall :  0.8  f1score:  0.7111111111111111\n",
            "34 :  64.0   precision :  0.9411764705882353  recall :  0.64  f1score:  0.7619047619047621\n",
            "35 :  69.56521739130434   precision :  0.8  recall :  0.6956521739130435  f1score:  0.7441860465116279\n",
            "36 :  64.0   precision :  0.9411764705882353  recall :  0.64  f1score:  0.7619047619047621\n",
            "37 :  80.0   precision :  0.8695652173913043  recall :  0.8  f1score:  0.8333333333333333\n",
            "38 :  47.05882352941176   precision :  0.6153846153846154  recall :  0.47058823529411764  f1score:  0.5333333333333333\n",
            "39 :  82.35294117647058   precision :  0.4827586206896552  recall :  0.8235294117647058  f1score:  0.608695652173913\n",
            "40 :  94.73684210526315   precision :  0.6923076923076923  recall :  0.9473684210526315  f1score:  0.7999999999999999\n",
            "41 :  87.5   precision :  0.7368421052631579  recall :  0.875  f1score:  0.7999999999999999\n",
            "42 :  93.33333333333333   precision :  0.9333333333333333  recall :  0.9333333333333333  f1score:  0.9333333333333333\n",
            "43 :  59.25925925925925   precision :  1.0  recall :  0.5925925925925926  f1score:  0.7441860465116279\n",
            "44 :  52.17391304347826   precision :  1.0  recall :  0.5217391304347826  f1score:  0.6857142857142856\n",
            "45 :  87.5   precision :  1.0  recall :  0.875  f1score:  0.9333333333333333\n",
            "46 :  92.85714285714286   precision :  0.65  recall :  0.9285714285714286  f1score:  0.7647058823529412\n",
            "47 :  57.14285714285714   precision :  0.9230769230769231  recall :  0.5714285714285714  f1score:  0.7058823529411765\n",
            "48 :  77.77777777777779   precision :  0.7  recall :  0.7777777777777778  f1score:  0.7368421052631577\n",
            "49 :  95.23809523809523   precision :  1.0  recall :  0.9523809523809523  f1score:  0.975609756097561\n",
            "50 :  66.66666666666666   precision :  0.9090909090909091  recall :  0.6666666666666666  f1score:  0.7692307692307692\n",
            "51 :  82.6086956521739   precision :  0.8260869565217391  recall :  0.8260869565217391  f1score:  0.8260869565217391\n",
            "52 :  80.0   precision :  0.8  recall :  0.8  f1score:  0.8000000000000002\n",
            "53 :  73.91304347826086   precision :  1.0  recall :  0.7391304347826086  f1score:  0.85\n",
            "54 :  61.53846153846154   precision :  1.0  recall :  0.6153846153846154  f1score:  0.761904761904762\n",
            "55 :  95.65217391304348   precision :  0.7857142857142857  recall :  0.9565217391304348  f1score:  0.8627450980392156\n",
            "56 :  68.18181818181817   precision :  0.9375  recall :  0.6818181818181818  f1score:  0.7894736842105263\n",
            "57 :  88.46153846153845   precision :  0.8846153846153846  recall :  0.8846153846153846  f1score:  0.8846153846153846\n",
            "58 :  86.36363636363636   precision :  1.0  recall :  0.8636363636363636  f1score:  0.9268292682926829\n",
            "59 :  71.42857142857143   precision :  0.7142857142857143  recall :  0.7142857142857143  f1score:  0.7142857142857143\n",
            "60 :  61.111111111111114   precision :  0.6111111111111112  recall :  0.6111111111111112  f1score:  0.6111111111111112\n",
            "61 :  95.23809523809523   precision :  0.5263157894736842  recall :  0.9523809523809523  f1score:  0.6779661016949152\n",
            "62 :  76.47058823529412   precision :  0.7222222222222222  recall :  0.7647058823529411  f1score:  0.7428571428571428\n",
            "63 :  84.21052631578947   precision :  0.6956521739130435  recall :  0.8421052631578947  f1score:  0.761904761904762\n",
            "64 :  75.0   precision :  0.9  recall :  0.75  f1score:  0.8181818181818182\n",
            "65 :  52.94117647058824   precision :  0.6  recall :  0.5294117647058824  f1score:  0.5625\n",
            "66 :  90.0   precision :  0.9  recall :  0.9  f1score:  0.9\n",
            "67 :  82.6086956521739   precision :  0.7916666666666666  recall :  0.8260869565217391  f1score:  0.8085106382978724\n",
            "68 :  80.76923076923077   precision :  0.5675675675675675  recall :  0.8076923076923077  f1score:  0.6666666666666666\n",
            "69 :  91.30434782608695   precision :  0.7241379310344828  recall :  0.9130434782608695  f1score:  0.8076923076923076\n",
            "70 :  80.95238095238095   precision :  0.7727272727272727  recall :  0.8095238095238095  f1score:  0.7906976744186046\n",
            "71 :  87.5   precision :  0.6086956521739131  recall :  0.875  f1score:  0.717948717948718\n",
            "72 :  81.25   precision :  0.7647058823529411  recall :  0.8125  f1score:  0.787878787878788\n",
            "73 :  82.75862068965517   precision :  0.8275862068965517  recall :  0.8275862068965517  f1score:  0.8275862068965517\n",
            "74 :  57.89473684210527   precision :  0.8461538461538461  recall :  0.5789473684210527  f1score:  0.6875\n",
            "75 :  75.0   precision :  0.9230769230769231  recall :  0.75  f1score:  0.8275862068965517\n",
            "76 :  91.30434782608695   precision :  0.9130434782608695  recall :  0.9130434782608695  f1score:  0.9130434782608695\n",
            "77 :  75.0   precision :  0.7894736842105263  recall :  0.75  f1score:  0.7692307692307692\n",
            "78 :  81.81818181818183   precision :  0.72  recall :  0.8181818181818182  f1score:  0.7659574468085107\n",
            "79 :  90.0   precision :  0.8181818181818182  recall :  0.9  f1score:  0.8571428571428572\n",
            "80 :  78.94736842105263   precision :  0.6818181818181818  recall :  0.7894736842105263  f1score:  0.7317073170731707\n",
            "81 :  82.35294117647058   precision :  0.56  recall :  0.8235294117647058  f1score:  0.6666666666666666\n",
            "82 :  59.09090909090909   precision :  0.6190476190476191  recall :  0.5909090909090909  f1score:  0.6046511627906977\n",
            "83 :  65.21739130434783   precision :  0.7142857142857143  recall :  0.6521739130434783  f1score:  0.6818181818181819\n",
            "84 :  79.16666666666666   precision :  0.95  recall :  0.7916666666666666  f1score:  0.8636363636363635\n",
            "85 :  83.33333333333334   precision :  1.0  recall :  0.8333333333333334  f1score:  0.9090909090909091\n",
            "86 :  80.0   precision :  1.0  recall :  0.8  f1score:  0.888888888888889\n",
            "87 :  85.71428571428571   precision :  1.0  recall :  0.8571428571428571  f1score:  0.923076923076923\n",
            "88 :  82.35294117647058   precision :  0.7  recall :  0.8235294117647058  f1score:  0.7567567567567567\n",
            "89 :  86.20689655172413   precision :  0.6944444444444444  recall :  0.8620689655172413  f1score:  0.7692307692307692\n",
            "90 :  80.0   precision :  0.8  recall :  0.8  f1score:  0.8000000000000002\n",
            "91 :  92.85714285714286   precision :  0.6190476190476191  recall :  0.9285714285714286  f1score:  0.742857142857143\n",
            "92 :  50.0   precision :  0.6153846153846154  recall :  0.5  f1score:  0.5517241379310345\n",
            "93 :  85.0   precision :  0.7083333333333334  recall :  0.85  f1score:  0.7727272727272727\n",
            "94 :  56.00000000000001   precision :  1.0  recall :  0.56  f1score:  0.717948717948718\n",
            "95 :  79.16666666666666   precision :  0.8636363636363636  recall :  0.7916666666666666  f1score:  0.8260869565217391\n",
            "96 :  88.88888888888889   precision :  0.6666666666666666  recall :  0.8888888888888888  f1score:  0.761904761904762\n",
            "97 :  66.66666666666666   precision :  0.6666666666666666  recall :  0.6666666666666666  f1score:  0.6666666666666666\n",
            "98 :  83.33333333333334   precision :  0.7692307692307693  recall :  0.8333333333333334  f1score:  0.8\n",
            "99 :  73.07692307692307   precision :  0.6785714285714286  recall :  0.7307692307692307  f1score:  0.7037037037037038\n"
          ]
        }
      ],
      "source": [
        "# evaluate trained models\n",
        "# find precision, recall, f1 score\n",
        "\n",
        "num_classes = 100 # no. of classes\n",
        "model.load_weights('%_on_100_class_CNN.h5') # load trained model\n",
        "\n",
        "\n",
        "def maxNumber(arr) : # find index of max number in array\n",
        "    mx = 0\n",
        "    mxIndx = -1\n",
        "    for indx, val in enumerate(arr):\n",
        "        if val > mx:\n",
        "            mx = val\n",
        "            mxIndx = indx\n",
        "    return mxIndx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "total = []    # total sample in each class\n",
        "pos_reg = []  # number of positive samples in each class\n",
        "fal_reg = []  # number of failed samples in each class\n",
        "\n",
        "for i in range(num_classes): # initializing\n",
        "    total.append(0)\n",
        "    pos_reg.append(0)\n",
        "    fal_reg.append(0)\n",
        "\n",
        "\n",
        "pred = model.predict(X_test) # predict on the test set\n",
        "\n",
        "for indx, res in enumerate(pred) :\n",
        "\n",
        "    target = int(y_test[indx])\n",
        "    \n",
        "    total[target] += 1\n",
        "    \n",
        "    if target == maxNumber(res) :\n",
        "        pos_reg[target] += 1\n",
        "    \n",
        "    else:\n",
        "        fal_reg[maxNumber(res)] += 1\n",
        "\n",
        "\n",
        "s1 = 0 \n",
        "s2 = 0 \n",
        "s3 = 0\n",
        "for i in range(num_classes):\n",
        "    s1 += pos_reg[i]\n",
        "    s2 += total[i]\n",
        "    s3 += fal_reg[i]\n",
        "\n",
        "print(\"Precision, Recall, F1-score on test dataset\")\n",
        "print(\"precision: \", s1/(s1+s3),\" recall: \", s1/s2, \"f1 score :\", (2*(s1/(s1+s3)))*(s1/s2)/((s1/(s1+s3))+(s1/s2)))\n",
        "\n",
        "\n",
        "print(\"Precision, Recall, F1-score on each class of test dataset\")\n",
        "for i in range(num_classes):\n",
        "    print(i, \": \", (pos_reg[i] / total[i] * 100), \"  precision : \", pos_reg[i]/(pos_reg[i]+fal_reg[i]), \" recall : \", pos_reg[i]/total[i], \" f1score: \", (2*(pos_reg[i]/(pos_reg[i]+fal_reg[i]))*(pos_reg[i]/total[i]))/((pos_reg[i]/(pos_reg[i]+fal_reg[i]))+(pos_reg[i]/total[i])))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "100_class_CNN.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "560ab4e8cf0ee1f6a014e2615ef8e4079d0c55d8f2bb773afc5b9b68a878d3eb"
    },
    "kernelspec": {
      "display_name": "Python 3.8.0 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
